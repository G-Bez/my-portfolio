[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Portfolio",
    "section": "",
    "text": "See more posts in Archive…\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nCase study - Average temperature in Europe from 2011 to 2022\n\n\n6 min\n\n\n\nR\n\n\nGIS\n\n\nVisualization\n\n\nCase study\n\n\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnimated plots in R with gganimate package\n\n\n8 min\n\n\n\nR\n\n\nVisualization\n\n\n\n\nNov 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear regression diagnostics in R with lindia package\n\n\n14 min\n\n\n\nR\n\n\nRegression\n\n\n\n\nNov 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGIS with R - Coordinate Reference Systems in R\n\n\n7 min\n\n\n\nR\n\n\nGIS\n\n\n\n\nNov 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGIS with R - Raster data\n\n\n4 min\n\n\n\nR\n\n\nGIS\n\n\n\n\nNov 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGIS with R - Vector data\n\n\n7 min\n\n\n\nR\n\n\nGIS\n\n\n\n\nNov 6, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData cleansing with Janitor R package\n\n\n4 min\n\n\n\nR\n\n\nData cleansing\n\n\n\n\nNov 3, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nColorblind palettes in R with viridisLite package\n\n\n3 min\n\n\n\nR\n\n\nVisualization\n\n\nAccessibility\n\n\n\n\nOct 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDependency management with renv R package\n\n\n4 min\n\n\n\nR\n\n\nDependency management\n\n\n\n\nOct 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCase study - Cyclistic\n\n\n20 min\n\n\n\nR\n\n\nCase study\n\n\n\n\nOct 19, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\nCategories\n\n\n\n\n\n\nNov 22, 2022\n\n\nCase study - Average temperature in Europe from 2011 to 2022\n\n\n5 min\n\n\nR,GIS,Visualization,Case study\n\n\n\n\nNov 19, 2022\n\n\nAnimated plots in R with gganimate package\n\n\n7 min\n\n\nR,Visualization\n\n\n\n\nNov 17, 2022\n\n\nLinear regression diagnostics in R with lindia package\n\n\n13 min\n\n\nR,Regression\n\n\n\n\nNov 9, 2022\n\n\nGIS with R - Coordinate Reference Systems in R\n\n\n6 min\n\n\nR,GIS\n\n\n\n\nNov 8, 2022\n\n\nGIS with R - Raster data\n\n\n3 min\n\n\nR,GIS\n\n\n\n\nNov 6, 2022\n\n\nGIS with R - Vector data\n\n\n6 min\n\n\nR,GIS\n\n\n\n\nNov 3, 2022\n\n\nData cleansing with Janitor R package\n\n\n3 min\n\n\nR,Data cleansing\n\n\n\n\nOct 30, 2022\n\n\nColorblind palettes in R with viridisLite package\n\n\n2 min\n\n\nR,Visualization,Accessibility\n\n\n\n\nOct 29, 2022\n\n\nDependency management with renv R package\n\n\n3 min\n\n\nR,Dependency management\n\n\n\n\nOct 19, 2022\n\n\nCase study - Cyclistic\n\n\n19 min\n\n\nR,Case study\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/gganimate/index.html",
    "href": "posts/gganimate/index.html",
    "title": "Animated plots in R with gganimate package",
    "section": "",
    "text": "If you ever thought static plots to be a bit bland or not enough powerful to represent all the nuances of your data, then gganimate package is what you need to further improve your visualizations. gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. It does this by providing a range of new grammar classes that can be added to the plot object in order to customise how it should change with time.\n\ntransition_*() defines how the data should be spread out and how it relates to itself across time.\nview_*() defines how the positional scales should change along the animation.\nshadow_*() defines how data from other points in time should be presented in the given point in time.\nenter_*()/exit_*() defines how new data should appear and how old data should disappear during the course of the animation.\nease_aes() defines how different aesthetics should be eased during transitions.\n\n\n\nInstall gganimate\n\nYou can install gganimate from CRAN repository with:\n\ninstall.packages(\"gganimate\")\n\nOr the the developer version from GitHub with:\n\ndevtools::install_github('thomasp85/gganimate')\n\n\n\n\nPackages\n\nWe start by loading the needed packages.\n\nlibrary(reshape2)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(gifski)\n\n\n\n\n\n\n\nNote:\n\n\n\nBy default, gganimate writes each animation frame to separate image files. and stops there. You can produce gif outputs by installing gifski package, or video outputs with av package. If installed, gganimate will automatically detect the rendering backend, but you can also manually choose the rendering method by setting the renderer parameter of gganimate::animate() to the desired value. See ?gganimate::animate() for more details.\n\n\nBefore proceding, I’ll explain some key concepts of animated data visualization.\n\n\n\nTweening\n\nAnimation has always been about the illusion of movement. Tweening is a major part of making that illusion look real. Tweening in animation is a short for inbetweening, and it’s the process of generating images that go between keyframes. Keyframes are the images at the beginning and end of a smooth transition.\nIn the context of data visualization, tweening is the process of splitting data into subsets based on a variable in the data, and calculating intermediary data states that ensures a smooth transition between the states. gganimate provides a wide range of different transitions, for different datatypes, defined by transition_*() functions. We’ll see some examples later.\n\n\n\nEasing\n\nIn the context of animation, easing lends a natural, organic movement to motion. A linear animation (one built without easing) will move mechanically without slowing down or speeding up. As this kind of consistency in speed doesn’t occur in nature, we perceive the movement as unnatural. Objects in real life don’t start moving and maintaining the same speed throughout. They start slowly, pick up speed and slow down as they come to a halt. That’s why gganimate supports the easing of animations with ease_aes() function.\nThere are plenty of different easing methods, defined by an easing function and by an easing modifier. For a list of available easing functions, see ?tweenr::display_ease (tweenr package is a dependency of gganimate). Below I explain the 3 different modifiers.\n\nIn: applies the easing function as-is.\nOut: applies the easing function in reverse.\nIn-Out: the first half of the transition is applied as-is, while the last half is applied in reverse.\n\nFor a visual representation of all the combinations of easing functions and modifiers, see easing.net webpage. We’ll see some examples later.\n\n\n\nLabeling\n\nIt can be quite hard to understand an animation without any indication as to what each point relates to. gganimate solves this by providing a set of variables for each frame, which can be inserted into plot labels using glue syntax.\n\n\n\nExample 1: mtcars dataset\n\nNow we’re ready for some actual animations. We start with the R prebuilt mtcars dataset.\n\ndatasets::mtcars\n\n\n\n  \n\n\n\nBelow a standard static plot of ‘mpg’ (miles per US gallon) variable by ‘cyl’ (number of cylinders) variable, built with ggplot2 . First we apply a custom theme.\n\n\nCustom theme\nggplot2::theme_update(\n  axis.text = element_text(size = 11),\n  legend.background = element_blank(),\n  panel.background = element_rect(fill = \"grey85\"),\n  panel.border = element_rect(colour = \"black\", fill = NA),\n  panel.grid = element_line(colour = \"whitesmoke\"),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.minor.x = element_blank(),\n  plot.title = element_text(hjust = 0.5),\n  plot.subtitle = element_text(face = \"italic\", hjust = 0.5),\n  strip.background = element_rect(colour = \"black\", fill = \"grey85\"),\n  strip.text = element_text(size = 10, face = \"bold\"),\n  title = element_text(size = 12, face = \"bold\")\n)\n\n\n\nh &lt;- ggplot(mtcars, aes(y = as.factor(cyl), x = mpg)) +\n  geom_boxplot(aes(fill = as.factor(cyl)), show.legend = F) +\n  scale_x_continuous(limits = c(10, 35), breaks = seq(10,35,2.5)) +\n  labs(title = \"Miles per gallon (mpg) by num. of cylinders (cyl)\", y = \"cyl\")\nh\n\n\n\n\nNow let’s say we want to see how the transmission type (‘am’ variable) influences the miles per gallon for each amount of cylinders. We can add an animation based on ‘am’ variable, which is categorical/binary (0 for automatic transmission, and 1 for manual transmission) with gganimate::transition_states() . This transition splits your data into multiple states based on the levels in a given column. We add the transition by simply adding a new layer to our plot, like we do in base ggplot2 .\n\nh_anim &lt;- h +\n  transition_states(states = as.factor(am), transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\") +\n  labs(subtitle = \"Transmission, am = {closest_state}\")\nh_anim\n\n\n\n\nAs you can see, the manual transmission (‘am’ = 1) is associated with a lower fuel consumption, on average, for each amount of cylinders.\nWith transition_length parameter we set the relative length of the transition between different states, while the relative length of the pause at each state is defined by state_length parameter. For a better animation, the easing modifier in ease_aes() was set to “in-out” for a slower start and end of each transition. Finally, a subtitle with glue syntax was added, so that the label changes whenever there’s a state transition. For a list of the available label variables for transition_states() function, see its documentation with ?gganimate::transition_states .\n\n\n\n\n\n\nCaution:\n\n\n\nThe glue syntax is automatically detected only when used inside ggplot2 labels functions (for example ggtitle() , labs() , etc.\n\n\nWe can also change the rendering parameters with animate() function. For example, below we set the number of frames to 200 (default = 100) for a smoother animation.\n\nh_anim |&gt; animate(nframes = 200)\n\n\n\n\n\n\n\nExample 2: iris dataset\n\nNow we’ll build an additional visualization on iris R prebuilt dataset.\n\ndatasets::iris\n\n\n\n  \n\n\n\nLet’s say we want to see what iris species have the longest and largest petals. We can draw a simple scatterplot to find out the “biggest” species.\n\ns &lt;- ggplot(iris, aes(x = Petal.Width, y = Petal.Length)) +\n  geom_point(aes(colour = Species)) +\n  labs(title = \"Petal.length vs Petal.Width\")\ns\n\n\n\n\nNow, this particular plot is clear and straightforward, and already fine as it is. Each species takes a specific area in the plot, and there’s almost no overlapping of data points. However, overlapping is a very common problem of scatter plots, especially when there are many data points, and creating an animation with different states can solve it, so let’s animate our iris plot.\n\ns_anim &lt;- s +\n  transition_states(Species, transition_length = 2) +\n  ease_aes(\"quadratic-in-out\") +\n  enter_fade() +\n  exit_shrink() +\n  labs(subtitle = \"Species = {closest_state}\") +\n  theme(legend.position = \"none\")\ns_anim\n\n\n\n\nThis time, custom enter and exit behaviours were set. In iris dataset, each row represents a different individual, there aren’t multiple observations of the same individual, so we don’t want the data points to move across the plot (default behaviour). If that was the case, it would appear as if data in a single measurement changes gradually as the flower being measured on somehow morphs between three different iris species, which obviously doesn’t make sense. To avoid this problem, an exit_shrink() call was added, so that the data points shrink in size and disappear when there’s a transition between states, rather than moving. Here, enter_fade() is just an aesthetic improvement (the data points start at max transparency and end up with the transparency level you set when building the plot; no transparency in this case).\n\n\n\nExample 3: USPersonalExpenditure dataset\n\nAs final example, we’ll build an animation on a custom long version of USPersonalExpenditure R prebuilt dataset.\n\nlong_USPE &lt;- function() {\n  df &lt;- datasets::USPersonalExpenditure |&gt; as.data.frame()\n  df$category &lt;- rownames(df)\n  df &lt;- reshape2::melt(df, \"category\")\n  df$variable &lt;- ordered(df$variable)\n  names(df) &lt;- c(\"category\", \"year\", \"expenditure\")\n  return(df)\n}\nlong_USPE()\n\n\n\n  \n\n\n\nFirst, we build a static visualization.\n\nl &lt;- ggplot(long_USPE(), aes(x = year, y = expenditure, group = category, colour = category)) +\n  geom_line() +\n  geom_point() +\n  labs(title = \"US personal expenditures\", y = \"Expenditure (billions $)\", x = \"Year\")\nl\n\n\n\n\nThe line plot here is already fine, and is a good representation of the increasing trends of personal expenditures from 1940 to 1960, but we can make it more visually appealing by adding a transition_reveal() call, which allows the data to gradually appear, along a given time dimension.\n\nl_anim &lt;- l +\n  geom_text(aes(label = expenditure |&gt; round(2) |&gt; format(nsmall = 2)), fontface = \"bold\", parse = T, \n            check_overlap = T, nudge_x = .2, nudge_y = 1) +\n  transition_reveal(as.numeric(year)) +\n  ease_aes(\"quadratic-in-out\") +\n  theme(legend.position = c(0.15,0.8), legend.title = element_blank())\nl_anim"
  },
  {
    "objectID": "posts/avgtemp/index.html",
    "href": "posts/avgtemp/index.html",
    "title": "Case study - Average temperature in Europe from 2011 to 2022",
    "section": "",
    "text": "In this brief case study I’ll analyze average temperatures in Europe from 2011 to 2022 (up to June 2022) and build an animated raster plot of temperatures with terra , ggplot2 and gganimate packages.\n\n\nPackages\n\nWe start by loading the needed packages\n\n\nlibrary(data.table) # fast data manipulation for big datasets\nlibrary(reshape2)\nlibrary(terra)\nlibrary(xts)      # packages for time \nlibrary(tsbox)    # series objects\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(RColorBrewer)\n\n\n\nE-OBS dataset\n\nOur data source is a subset of the open E-OBS dataset provided by Copernicus Climate Change Service (C3S). To manually access the data, open the E-OBS link I just provided, scroll down until you see ‘Access to smaller chunks’ section, and click on the corresponding link. Then, find the 2011-2022 subset of the 0.25 degree resolution dataset and click on ‘TG’ (which stands for daily mean temperature). Here I choose the lower resolution version since it’s more lightweight. Do the same for the 0.1 degree resolution if you want more precise data.\nHere I’ll directly scrape the dataset from Copernicus website.\n\nUrl &lt;- \"https://knmi-ecad-assets-prd.s3.amazonaws.com/ensembles/data/Grid_0.25deg_reg_ensemble/tg_ens_mean_0.25deg_reg_2011-2022_v26.0e.nc\"\ntemp &lt;- tempfile(fileext = \".nc\")\noptions(timeout = 5000); download.file(url = Url, temp, mode = \"wb\"); options(timeout = 60)\n\nrt &lt;- terra::rast(temp)\nrt\n\nclass       : SpatRaster \ndimensions  : 201, 464, 4199  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -40.5, 75.5, 25.25, 75.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 \nsource      : file258c47a32c99.nc \nvarname     : tg (mean temperature) \nnames       :    tg_1,    tg_2,    tg_3,    tg_4,    tg_5,    tg_6, ... \nunit        : Celsius, Celsius, Celsius, Celsius, Celsius, Celsius, ... \ntime (days) : 2011-01-01 to 2022-06-30 \n\n\nThese are daily data ranging from 2011-01-01 to 2022-06-30.\n\n\n\nExtract monthly subset\n\nSince the object contains more than 80 millions cells (201 rows, 464 columns and 4199 layers), I’ll extract a subset (a single day for each month) in order to improve the rendering time.\nFirst, in order to plot the data, the SpatRaster object must be converted into a data.frame-like format. Here I’ll convert it to data.table format for better performance.\n\ndt &lt;- as.data.table(rt, xy = T)\n\ndaily_range &lt;- seq.Date(from = as.Date(\"2011-01-01\"), to = as.Date(\"2022-06-30\"), by = \"day\") |&gt;\n  as.character()\nnames(dt) &lt;- c(\"x\", \"y\", daily_range)\n\ndim(dt)\n\n[1] 19902  4201\n\n\nThe result is a data.table with 19.902 rows and 4201 columns, the first two being the x and y coordinates of raster grid cells, and the other ones being the grid cells values for each day. Then, all column names, except the first two, where changed to match their corresponding dates.\nNow, since the wide format isn’t ggplot2-friendly, the data must be converted to long format, with reshape2::melt() function.\n\ndt_long &lt;- melt(dt, id.vars = c(\"x\",\"y\")) |&gt; as.data.table() |&gt; setkeyv(c(\"x\", \"y\"))\nnames(dt_long) &lt;- c(\"x\", \"y\", \"date\", \"value\")\nstr(dt_long)\n\nClasses 'data.table' and 'data.frame':  83568498 obs. of  4 variables:\n $ x    : num  -24.9 -24.9 -24.9 -24.9 -24.9 ...\n $ y    : num  69.4 69.4 69.4 69.4 69.4 ...\n $ date : Factor w/ 4199 levels \"2011-01-01\",\"2011-01-02\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ value: num  NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr [1:2] \"x\" \"y\"\n\n\nFinally, we can extract the monthly subset.\n\nmonthly_range &lt;- seq.Date(from = as.Date(\"2011-01-01\"), to = as.Date(\"2022-06-30\"), by = \"month\") |&gt; \n  as.character()\n\ndt_monthly &lt;- dt_long[date %in% monthly_range]\ndt_monthly$date &lt;- as.Date(dt_monthly$date)\n\nstr(dt_monthly)\n\nClasses 'data.table' and 'data.frame':  2746476 obs. of  4 variables:\n $ x    : num  -24.9 -24.9 -24.9 -24.9 -24.9 ...\n $ y    : num  69.4 69.4 69.4 69.4 69.4 ...\n $ date : Date, format: \"2011-01-01\" \"2011-02-01\" ...\n $ value: num  NA NA NA NA NA NA NA NA NA NA ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr [1:2] \"x\" \"y\"\n\n\nThis final subset has 2.746.476 rows, way less than the 83.568.498 rows in dt_long. This should provide a good performance improvement.\n\n\n\nA quick time series decomposition\n\nBefore building the animated plot, let’s do a quick time series decomposition to see if the trend component of our daily data is increasing. First, we summarize our daily means (i.e. we compute means of raster pixels values for each day; means of avg. temperatures in whole Europe).\n\nsumm &lt;- dt_long[, .(mean = mean(value, na.rm = T)), by = date]\nsumm$date &lt;- as.Date(summ$date)\nsumm\n\n\n\n  \n\n\n\nThen we convert ‘summ’ into a time series object, that can be used with the decomposition function stl() .\n\nts &lt;- xts::xts(summ$mean, order.by = summ$date) |&gt; tsbox::ts_ts()\nstr(ts)\n\n Time-Series [1:4199] from 2011 to 2022: -2.17 -1.96 -3.62 -5.27 -5.17 ...\n\n\nFinally we plot the the time series decomposition.\n\nstl(ts, s.window = 365) |&gt; \n  plot(main = \"Time series decomposition of avg. daily temp. in Europe\\nFrom 2011-01-01 to 2022-30-06\")\n\n\n\n\nThere’s a clear increasing trend from 8°C ca. up to more than 9°C, from 2011 to start of 2020, that later reverts back to 2011-2012 values.\n\n\n\nChoose a color palette\n\nBefore building the animated plot, we should choose a good color palette. Since we’re dealing with temperature data in degrees Celsius (which have a 0 value), a diverging palette is ideal. RColorBrewer package offers many diverging colorblind-friendly palettes. Show the available palettes with RColorBrewer::display.brewer.all() function.\n\ndisplay.brewer.all(type = \"div\", colorblindFriendly = T)\n\n\n\n\nThe first palette (RdYlBu) is a perfect choice for temperature data, so we’ll stick with it in the following visualizations.\n\n\n\nStatic example\n\nBefore rendering the actual animation, let’s try to plot a static chart for a specific date (for example 2011-01-01). First, we set a custom ggplot2 dark theme, for better contrast with RColorBrewer palette.\n\n\nCustom theme\ntheme_update(\n  axis.text = element_text(size = 9, colour = \"whitesmoke\"),\n  axis.ticks = element_line(colour = \"whitesmoke\"),\n  legend.background = element_blank(),\n  panel.background = element_rect(fill = \"gray15\"),\n  panel.border = element_rect(colour = \"whitesmoke\", fill = NA),\n  panel.grid.major = element_line(colour = \"grey10\"),\n  panel.grid.minor = element_blank(),\n  plot.background = element_rect(fill = \"gray10\"),\n  plot.title = element_text(hjust = 0.5),\n  plot.subtitle = element_text(face = \"italic\", hjust = 0.5),\n  strip.background = element_rect(colour = \"gray10\", fill = \"grey10\"),\n  strip.text = element_text(size = 9, face = \"bold\"),\n  title = element_text(size = 10, face = \"bold\"),\n  text = element_text(colour = \"whitesmoke\")\n)\n\n\nThen we use ggplot2::geom_raster() to plot the data.\n\nstatic &lt;- ggplot(dt_monthly[date == \"2011-01-01\"], aes(x, y, fill = value)) +\n  geom_raster(interpolate = T) +\n  coord_sf(crs = 4326) +\n  scale_fill_gradientn( colours = brewer.pal(11, \"RdYlBu\") |&gt; rev() ) +\n  labs(x = \"lon\", y = \"lat\", title = \"Avg. temperature\", subtitle = \"Date: 2011-01-01\", fill = \"Temp. (C°)\")\nstatic\n\n\n\n\nUnfortunately there are some missing cells for Turkey and North Africa; anyway we got a nice static map of average temperatures in 2011-01-01.\n\n\n\nAnimated plot\n\nFinally let’s see how temperatures change over time with an animated plot. See Animated plots in R with gganimate package for a quick introduction to gganimate package.\n\nanim &lt;- ggplot(dt_monthly, aes(x, y, fill = value)) +\n  geom_raster(interpolate = T) +\n  scale_fill_gradientn( colours = brewer.pal(11, \"RdYlBu\") |&gt; rev() ) +\n  transition_states(as.character(date)) +\n  ease_aes(\"quadratic-in-out\") +\n  labs(x = \"lon\", y = \"lat\", title = \"Avg. temperature\", subtitle = \"Date: {closest_state}\")\n\nanimate(anim, nframes = length(monthly_range)*4+50, end_pause = 50, duration = 90)\n\n\n\n\n\n\nUnfortunately, due to the limitations of my personal hardware I can’t render smoother and more detailed animations. Anyway this is still a nice showcase of gganimate and spatial packages working together in R.\n\n\n\nAcknowledgments\n\n“I acknowledge the E-OBS dataset from the EU-FP6 project UERRA (https://www.uerra.eu) and the Copernicus Climate Change Service, and the data providers in the ECA&D project (https://www.ecad.eu)”\nCornes, R., G. van der Schrier, E.J.M. van den Besselaar, and P.D. Jones. 2018: An Ensemble Version of the E-OBS Temperature and Precipitation Datasets, J. Geophys. Res. Atmos., 123. doi:10.1029/2017JD028200”"
  },
  {
    "objectID": "posts/renv_get_started/index.html#introduction",
    "href": "posts/renv_get_started/index.html#introduction",
    "title": "Dependency management with renv R package",
    "section": "Introduction",
    "text": "Introduction\n\nThe renv package enables project-local dependency management to your RStudio projects, and is meant as a stable replacement for the older Packrat package. Basically, renv helps the user in making his projects and workflows consistent, robust, reproducible and portable, and ensures they keep working by managing library paths and isolating their dependencies. More easily, renv gives a private package library to each project. If you ever had troubles in figuring out what packages you need to install to make someone else’s code work, or some of your older projects stopped working after updating your global library, renv is the tool you need. For more details see renv webpage."
  },
  {
    "objectID": "posts/renv_get_started/index.html#installation",
    "href": "posts/renv_get_started/index.html#installation",
    "title": "Dependency management with renv R package",
    "section": "Installation",
    "text": "Installation\n\nYou can install the latest version from CRAN with:\n\n\ninstall.packages(\"renv\")\n\n\nOr the developer version from the R-universe:\n\n\ninstall.packages(\"renv\", repos = \"https://rstudio.r-universe.dev\")"
  },
  {
    "objectID": "posts/renv_get_started/index.html#example-project",
    "href": "posts/renv_get_started/index.html#example-project",
    "title": "Dependency management with renv R package",
    "section": "Example project",
    "text": "Example project\n\nTo show how renv actually works, we create a new RStudio project with File &gt; New Project ... &gt; New directory (see Figure 1).\n\n\n\nFigure 1: Create new project\n\n\nAs you can see, after installing renv, a dedicated tickbox is added to new projects windows. Clicking on it will automatically initialize renv when creating the project. Here we leave it unchecked, to show how to manually initialize the dependency management. We call the new project ‘renv-test-project’. Below you can see its main directory (Figure 2).\n\n\n\nFigure 2: New project main dir\n\n\nSince we initialized a blank project, we only have the project main file ‘renv-test-project.Rproj’, and user data inside the ‘.Rproj.user’ folder."
  },
  {
    "objectID": "posts/renv_get_started/index.html#initialize-renv",
    "href": "posts/renv_get_started/index.html#initialize-renv",
    "title": "Dependency management with renv R package",
    "section": "Initialize renv",
    "text": "Initialize renv\n\nTo manually initialize a new project-local environment with a private library for our ‘renv-test-project’, we run:\n\n\nrenv::init()\n\n\nrenv::init() scans the project and looks for dependencies in your code, then creates a private package library, and saves its state in a text file called ‘renv.lock’ (the file contains records of packages versions, external sources, if known, repositories, and a hash assigned to each package). It also writes out the infrastructure necessary to automatically load and use the private library for new R sessions launched from the project directory.\n\n\n\nFigure 3: Main dir after renv initialization\n\n\nAs you can see, some files and a folder were added to the project main directory (see Figure 3). ‘renv’ folder includes the private library, and part of the infrastructure we mentioned earlier. ‘renv.lock’ file is the most important part of renv dependency management; it can be used to restore a previous state of the project, or to reinitialize the private library from scratch (useful when sharing the project).\nYou can see the current project private library in renv/library/R-x.x/x86_64-w64mingw32 folder (Figure 4). Since our project is currently blank, only renv package itself was added to our library.\n\n\n\nFigure 4: Initial private library"
  },
  {
    "objectID": "posts/renv_get_started/index.html#update-the-private-library",
    "href": "posts/renv_get_started/index.html#update-the-private-library",
    "title": "Dependency management with renv R package",
    "section": "Update the private library",
    "text": "Update the private library\n\nAfter initializing renv you can work in the project as you would normally do. You can write code and install and remove the necessary packages/dependencies.\n\n\n\n\n\n\n\nNote:\n\n\n\nWhen running renv the base install.packages() and remove.packages() functions are masked by their renv counterparts.\n\n\n\nLet’s say for example we need tibble package for our analysis. First, we install it.\n\n\ninstall.packages(\"tibble\")\n\n\ntibble and all of its dependencies are added to our private library (Figure 5).\n\n\n\nFigure 5: Private library after installing tibble\n\n\n\n\nThen we write a simple script that uses tibble package. For example, the following script assigns mtcars R prebuilt dataset to a tibble object.\n\n\nlibrary(tibble)\ntbl_mtcars &lt;- as_tibble(mtcars)\n\n\nWe save this script in our project main folder (Figure 6), but you can also place it in a dedicated subfolder, as you like.\n\n\n\nFigure 6: New script file\n\n\nNow, we run renv::snapshot() to add tibble package and its dependencies to our lockfile.\n\n\nrenv::snapshot()\n\n\nThe function scans the whole project to find new dependencies, and detects our new script file where we called tibble package. After finding out the new dependencies, it updates the lockfile."
  },
  {
    "objectID": "posts/renv_get_started/index.html#restore-a-previous-state",
    "href": "posts/renv_get_started/index.html#restore-a-previous-state",
    "title": "Dependency management with renv R package",
    "section": "Restore a previous state",
    "text": "Restore a previous state\n\nTo restore a previous project state from the lockfile, you just need to run:\n\n\nrenv::restore()\n\n\nrenv::init() can also restore a previous state. If a lockfile already exists, running renv::inits prompts some choices (see Figure 7).\n\n\n\nFigure 7: init() with existing lockfile\n\n\nChoice 1 is equivalent to renv::restore() . Choice 2 re-initializes the private library from scratch."
  },
  {
    "objectID": "posts/janitor/index.html#install-janitor",
    "href": "posts/janitor/index.html#install-janitor",
    "title": "Data cleansing with Janitor R package",
    "section": "Install janitor",
    "text": "Install janitor\n\nYou can install the most recent official version of janitor from CRAN with:\n\ninstall.packages(\"janitor\")\n\nOr the developer version from GitHub:\n\ndevtools::install_github(\"sfirke/janitor\")"
  },
  {
    "objectID": "posts/janitor/index.html#example-dataset",
    "href": "posts/janitor/index.html#example-dataset",
    "title": "Data cleansing with Janitor R package",
    "section": "Example dataset",
    "text": "Example dataset\n\nWe use a custom version of iris R prebuilt dataset, which I manually made ‘dirty’ by changing column names and adding some dirty columns and rows.\n\niris_custom &lt;- function() {\n  df &lt;- datasets::iris\n  df$Species &lt;- as.character(df$Species)\n  df[,6] &lt;- NA\n  df[,7] &lt;- 1\n  df[10,] &lt;- NA\n  names(df) &lt;- seq(1,7) |&gt; as.character()\n  dirty_names &lt;- c(\"Sepal length, (cm)\", \"Sepal width, (cm)\", \n                   \"Petal length, (cm)\", \"Petal width, (cm)\", \n                   \"species\", \"empty\", \"const\")\n  df &lt;- rbind(dirty_names, df)\n  return(df)\n}\n\niris_dirty &lt;- iris_custom()\niris_dirty\n\n\n\n  \n\n\n\nChanges done:\n\nConverted ‘species’ column to character.\nReplaced 10th row with NAs.\nAdded an empty column.\nAdded a constant column.\nChanged colnames to a sequence on numbers from 1 to 7.\nAdded dirty column names as first row.\n\nNow let’s see some janitor functions in action."
  },
  {
    "objectID": "posts/janitor/index.html#set-column-names-from-row",
    "href": "posts/janitor/index.html#set-column-names-from-row",
    "title": "Data cleansing with Janitor R package",
    "section": "Set column names from row",
    "text": "Set column names from row\n\nWith janitor::row_to_names() we can set our first row as column names.\n\nlibrary(janitor)\niris_dirty &lt;- janitor::row_to_names(iris_dirty, row_number = 1)\niris_dirty"
  },
  {
    "objectID": "posts/janitor/index.html#clean-column-names",
    "href": "posts/janitor/index.html#clean-column-names",
    "title": "Data cleansing with Janitor R package",
    "section": "Clean column names",
    "text": "Clean column names\n\nNow we need to clean those column names since they have a very bad format. janitor::clean_names() is a very useful and easy to use function that cleans badly formatted column names. You can choose to change all names to snake case (all lower case words, separated by underscores), variations on camel case (internal capital letters between words), title case or other styles. It can also be used to remove parts of names and any special characters, including replacing % symbols with the word ‘percent’. See snakecase::to_any_case() function documentation for a list of allowed cases. Below a few examples.\n\nsnake_case &lt;- janitor::clean_names(iris_dirty) |&gt; names() # default to snake case\nsnake_case\n\n[1] \"sepal_length_cm\" \"sepal_width_cm\"  \"petal_length_cm\" \"petal_width_cm\" \n[5] \"species\"         \"empty\"           \"const\"          \n\nbig.camel_case &lt;- janitor::clean_names(iris_dirty, case=\"big_camel\") |&gt; names()\nbig.camel_case\n\n[1] \"SepalLengthCm\" \"SepalWidthCm\"  \"PetalLengthCm\" \"PetalWidthCm\" \n[5] \"Species\"       \"Empty\"         \"Const\"        \n\nall.caps_case &lt;- janitor::clean_names(iris_dirty, case=\"all_caps\") |&gt; names()\nall.caps_case\n\n[1] \"SEPAL_LENGTH_CM\" \"SEPAL_WIDTH_CM\"  \"PETAL_LENGTH_CM\" \"PETAL_WIDTH_CM\" \n[5] \"SPECIES\"         \"EMPTY\"           \"CONST\"          \n\n\nNow let’s set iris_dirty colnames to default snake case:\n\niris_dirty &lt;- janitor::clean_names(iris_dirty)\niris_dirty"
  },
  {
    "objectID": "posts/janitor/index.html#remove-empty-columns-and-rows",
    "href": "posts/janitor/index.html#remove-empty-columns-and-rows",
    "title": "Data cleansing with Janitor R package",
    "section": "Remove empty columns and rows",
    "text": "Remove empty columns and rows\n\nI previously added empty rows and columns to the original iris dataset. Those can be removed with janitor::remove_empty() function.\n\niris_dirty &lt;- janitor::remove_empty(iris_dirty, quiet = F) # default removes both empty cols and rows\n\nvalue for \"which\" not specified, defaulting to c(\"rows\", \"cols\")\n\n\nRemoving 1 empty rows of 150 rows total (0.667%).\n\n\nRemoving 1 empty columns of 7 columns total (Removed: empty)."
  },
  {
    "objectID": "posts/janitor/index.html#remove-constant-column",
    "href": "posts/janitor/index.html#remove-constant-column",
    "title": "Data cleansing with Janitor R package",
    "section": "Remove constant column",
    "text": "Remove constant column\n\nThere’s also a function to remove columns with constant values, called janitor::remove_constant() . Previously I added to our dataset a ‘const’ column with all values equal to 1. Let’s remove it:\n\niris_dirty &lt;- janitor::remove_constant(iris_dirty, quiet = F)\n\nRemoving 1 constant columns of 6 columns total (Removed: const).\n\n\nYou can also set na.rm = T to ignore NA values when considering if a column is a constant."
  },
  {
    "objectID": "posts/janitor/index.html#frequency-and-contingency-tables",
    "href": "posts/janitor/index.html#frequency-and-contingency-tables",
    "title": "Data cleansing with Janitor R package",
    "section": "Frequency and contingency tables",
    "text": "Frequency and contingency tables\n\nBefore proceding, we set ‘iris_dirty’ columns to the correct datatypes, since previous manipulations changed all variables datatypes to character.\n\niris_dirty[,1:4] &lt;- lapply(iris_dirty[,1:4], as.numeric)\niris_dirty$species &lt;- as.factor(iris_dirty$species)\nstr(iris_dirty)\n\n'data.frame':   149 obs. of  5 variables:\n $ sepal_length_cm: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 5.4 ...\n $ sepal_width_cm : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.7 ...\n $ petal_length_cm: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ petal_width_cm : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.2 ...\n $ species        : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nWith janitor::tabyl() function we can produce frequency and contingency table. The main advantage of tabyl() , compared to R base table() function, is the former returns a data.frame object. The following code returns a frequency table with ‘species’ being the grouping variable.\n\nfreq_tbl &lt;- janitor::tabyl(iris_dirty, species)\nfreq_tbl\n\n\n\n  \n\n\n\nWe can also return a contingency table for species and petal width, for example:\n\ncon_tbl &lt;- janitor::tabyl(iris_dirty, petal_width_cm, species)\ncon_tbl"
  },
  {
    "objectID": "posts/janitor/index.html#formatting-tabulated-data",
    "href": "posts/janitor/index.html#formatting-tabulated-data",
    "title": "Data cleansing with Janitor R package",
    "section": "Formatting tabulated data",
    "text": "Formatting tabulated data\n\njanitor::adorn_*() functions can be used to format tabulated data; these functions are useful for reporting in Quarto and Markdown documents, when paired with knitr::kable() and similar functions. For example, adorn_pct_formatting() can be used to format the percentage output of tabyl() . Below, we round the percentages to 2 decimal places, and add the percentage sign at the end.\n\nfreq_tbl &lt;- janitor::adorn_pct_formatting(freq_tbl, digits = 2)\nfreq_tbl\n\n\n\n  \n\n\n\nBy default, the values in contingency tables are shown as counts. They can be changed to percentages with adorn_percentages() function.\n\ncon_tbl &lt;- janitor::adorn_percentages(con_tbl) |&gt; \n  janitor::adorn_pct_formatting(digits = 2)\ncon_tbl"
  },
  {
    "objectID": "posts/gis_raster/index.html#packages",
    "href": "posts/gis_raster/index.html#packages",
    "title": "GIS with R - Raster data",
    "section": "Packages",
    "text": "Packages\n\nWe start by loading the needed packages\n\n\nlibrary(terra)\nlibrary(ggplot2)\n\n\nterra package is a more modern replacement for raster package. It is simpler and faster (see a comparison of terra and raster packages. However, raster is not yet deprecated since there still might be compatibility issues between terra and other packages, so it might still be worth it to check its documentation."
  },
  {
    "objectID": "posts/gis_raster/index.html#creating-a-raster-layer",
    "href": "posts/gis_raster/index.html#creating-a-raster-layer",
    "title": "GIS with R - Raster data",
    "section": "Creating a raster layer",
    "text": "Creating a raster layer\n\nYou can use terra::rast() function to create a raster layer. See rast() function documentation for more details.\n\n\nrt &lt;- terra::rast(crs = \"+proj=longlat +datum=WGS84 +no_defs\", xmin = 30, \n                  xmax = 32, ymin = 44, ymax = 46.5, resolution = 0.05)\nrt\n\nclass       : SpatRaster \ndimensions  : 50, 40, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : 30, 32, 44, 46.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs \n\n\n\nWe just created a SpatRaster object, a rectangular grid cell with each cell being 0.05x0.05 degrees in size on WGS84 CRS. Below a description of the function parameters:\n\nresolution: spatial resolution; how large the pixels are.\nxmin, xmax: extent from east to west of the matrix, in meters.\nymin, ymax: extent from north to south of the matrix, in meters.\ncrs: coordinate reference system, in PROJ-4 notation. See GIS with R - Coordinate Reference Systems in R\n\nWe can also set the resolution indirectly with nrows and ncols parameters being the number of rows and columns, respectively. These two parameters are ignored if resolution parameter is used.\n\n\nrt &lt;- terra::rast(crs = \"+proj=longlat +datum=WGS84 +no_defs\", nrows = 50,\n                    ncols = 40, xmin = 30, xmax = 32, ymin = 44, ymax = 46.5)\nrt\n\nclass       : SpatRaster \ndimensions  : 50, 40, 1  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : 30, 32, 44, 46.5  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +datum=WGS84 +no_defs"
  },
  {
    "objectID": "posts/gis_raster/index.html#assign-values-to-raster-cells",
    "href": "posts/gis_raster/index.html#assign-values-to-raster-cells",
    "title": "GIS with R - Raster data",
    "section": "Assign values to raster cells",
    "text": "Assign values to raster cells\n\nThe RasterLayer object we just created is still blank. We can inspect the grid values with terra::values() function.\n\n\nterra::values(rt) |&gt; is.na() |&gt; all()\n\nWarning: [readValues] raster has no values\n\n\n[1] TRUE\n\n\n\nWe can retrieve the number of cells with terra::ncell() function.\n\n\nterra::ncell(rt)\n\n[1] 2000\n\n\n\nFinally, we can assign values with terra::values() function. Below we assign random values sampled from a normal distribution.\n\n\nterra::values(rt) &lt;- terra::ncell(rt) |&gt; rnorm(mean = 5, sd = 3)\nterra::values(rt)[1:100]\n\n  [1]  4.4577290  7.1305347 10.3989531 -0.6032462  7.9166603  2.0601587\n  [7]  4.3384531 -0.3273104 10.1728121  5.1359486  4.9309416  7.8454306\n [13]  4.4319885  7.7229082  1.4424075  6.0253073  1.4428289 11.8271790\n [19]  0.5123392  6.8357814  2.5527678  5.9824555  5.0171893 10.9063579\n [25]  6.8644344  6.6424489  6.7971675  2.2153942  6.9381177  6.0199055\n [31]  8.8837382  1.7735370  6.6240217 11.0126797  5.4060161  3.4311461\n [37]  6.5170732  6.3220853  2.3850320  8.3991728  6.6564697  9.5974992\n [43]  9.8672409  1.5868477  9.1637006  3.4236865  9.2520564  3.4599199\n [49]  0.3645040  9.5282144  7.7718261  2.6186751  7.8685675  7.1742571\n [55]  6.6027794  1.2702702  5.2060834  5.2090197  8.5525445  4.3295705\n [61]  5.7286017  6.8776796 -2.5368801  0.7480990  4.9483092  8.8282615\n [67]  6.1062462  9.7098303  8.5476771  4.5579887  5.1947192  8.7346013\n [73]  3.4851576  5.5246434  1.8211105  3.3071920  3.2760969  1.5672343\n [79]  7.5684717  6.6628725  5.3538841 10.0498555  2.4405985  6.7033253\n [85]  7.7156567  5.0507975  6.1396512  5.6348385  3.2400304  8.5612993\n [91]  8.9382107  3.0463443  6.3916331  3.6375107  4.9434139  3.0450383\n [97]  5.9685303  6.5675580  1.4278707  5.3673314\n\n\n\nAs you can see, the grid cells were properly filled."
  },
  {
    "objectID": "posts/gis_raster/index.html#plot-a-raster-layer",
    "href": "posts/gis_raster/index.html#plot-a-raster-layer",
    "title": "GIS with R - Raster data",
    "section": "Plot a raster layer",
    "text": "Plot a raster layer\n\nTo plot our RasterLayer we can use ggplot2::geom_rast() function. But first we need to assign its values to a data.frame.\n\n\ndf_rast &lt;- as.data.frame(rt, xy = T)\ndf_rast\n\n\n\n  \n\n\n\n\nWe use xy parameter to extract also the XY coordinates of each cell.\n\n\np &lt;- ggplot(df_rast, aes(x, y, fill = lyr.1)) +\n  geom_raster() +\n  labs(x = \"lon\", y = \"lat\", title = \"Normal distr. RasterLayer\")\np"
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#introduction",
    "href": "posts/Case_study_Cyclistic/index.html#introduction",
    "title": "Case study - Cyclistic",
    "section": "Introduction",
    "text": "Introduction\n\nIn this case study we will perform real-world tasks of a junior data analyst, working for a fictional bike sharing company, called Cyclistic. This case study is taken from Google data analytics professional certificate on Coursera.\nCyclistic’s director of marketing believes the company’s future success depends on maximizing the number of annual memberships. Therefore our (fictional) team wants to understand how casual riders and annual members differ. The final goal is to design a new marketing strategy to convert casuals into members, and more specifically, we need to answer two questions:\n\nHow Cyclistic can increase its revenue based on the available data?\nHow the marketing team can use social media to help with maximizing the number of members?\n\nBut first the marketing team recommendations must be backed up with compelling data insights and visualizations, in order to get approved by the management."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#packages",
    "href": "posts/Case_study_Cyclistic/index.html#packages",
    "title": "Case study - Cyclistic",
    "section": "Packages",
    "text": "Packages\n\nHere is the list of packages we’ll use throughout this case study.\n\ndata.table: Fast data manipulation.\nlubridate: Datetime tools.\ngeosphere: Spherical trigonometry for spatial applications.\nrstatix: Framework for basic statistical tests.\nrobustbase: Basic robust statistics.\nggplot2: Data visualization.\nggmap: Spatial visualizations with for ggplot2 users.\nviridisLite: Colorblind friendly color palettes.\npatchwork: Easy plot composition for ggplot2 users.\n\nWe start by loading these packages.\n\n\nlibrary(data.table)\nlibrary(lubridate)\nlibrary(geosphere)\nlibrary(rstatix)\nlibrary(robustbase)\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(viridisLite)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#dataset",
    "href": "posts/Case_study_Cyclistic/index.html#dataset",
    "title": "Case study - Cyclistic",
    "section": "Dataset",
    "text": "Dataset\n\nThe relevant historical bike trip data can be downloaded from this url (see license agreement). Here we use data ranging from June 2021 to May 2022 (archives from “202106-divvy-tripdata.zip” to “202205-divvy-tripdata.zip” included).\nEach archive contains a single monthly dataset, where each observation represents a unique bike ride. Each dataset has 13 columns/variables:\n\nride_id: alphanumeric id for each ride.\nrideable_type: Bike type. Categorical variable with 3 levels: “classic_bike”, “docked_bike”, “electric_bike”.\nmember_casual: Customer type. Categorical variable with 2 levels: “casual”, “member”.\nstarted_at: Ride start datetime.\nended_at: Ride end datetime.\nstart_station_name: Name of the start station.\nstart_station_id: Alphanumeric id for start station.\nend_station_name: Name of the end station.\nend_station_id: Alphanumeric id for end station.\nstart_lat: Start latitude coordinate.\nstart_lng: Start longitude coordinate.\nend_lat: End latitude coordinate.\nend_lng: End longitude coordinate."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#data-scraping-and-manipulation",
    "href": "posts/Case_study_Cyclistic/index.html#data-scraping-and-manipulation",
    "title": "Case study - Cyclistic",
    "section": "Data scraping and manipulation",
    "text": "Data scraping and manipulation\n\nAs first step, we scrape the datasets, merge them into a total dataset, and do some manipulation. In particular we add some additional columns, computed from the existing ones:\n\nride_length: Ride length in seconds (‘ended_at’ - ‘started_at’).\nyear_month: Year and month value extracted from ‘started_at’ variable.\nweekday: Day of week extracted from ‘started_at’ variable.\nride_dist: Mileage in meters. Computed from latitude and longitude values.\n\nThen we convert ‘year_month’ and ‘member_casual’ variables into factor datatype, and ‘year_month’ into ordered factor. To accomplish these tasks we use 3 custom functions.\n\n\n\nCustom scraping and manipulation functions\nadd.cols &lt;- function(dt) {\n  if(!(\"data.table\" %in% class(dt))) stop(\"dt is not a data.table object.\")\n  if(Sys.getlocale(\"LC_TIME\") != \"English_United States.1252\") Sys.setlocale(\"LC_TIME\", \"English\")\n  year_month &lt;- format(dt$started_at, \"%b %Y\")\n  weekday &lt;- lubridate::wday(dt$started_at, label = T, abbr = T, week_start = 1)\n  ride_length &lt;- as.numeric(dt$ended_at - dt$started_at)\n  ride_dist = geosphere::distGeo(p1 = dt[, c(\"start_lng\", \"start_lat\"), with = F], \n                                 p2 = dt[, c(\"end_lng\", \"end_lat\"), with = F])\n  return(cbind(dt, year_month, weekday, ride_length, ride_dist))\n}\n\n\nchange.datatypes &lt;- function(dt) {\n  if(!(\"data.table\" %in% class(dt))) stop(\"dt is not a data.table object.\")\n  dt[, c(\"rideable_type\", \"member_casual\")] &lt;- lapply(\n    dt[, c(\"rideable_type\", \"member_casual\")],\n    as.factor\n  )\n  dt[[\"year_month\"]] &lt;- ordered(dt[[\"year_month\"]], levels = unique(dt$year_month))\n  return(dt)\n}\n\n\nget.raw_data &lt;- function(Url, Timeout = 60) {   ## increase timeout if you get timeout error\n  mm &lt;- c(\"202106\", \"202107\", \"202108\", \"202109\", \"202110\", \"202111\", \"202112\", \"202201\",\n          \"202202\", \"202203\", \"202204\", \"202205\")\n  url1 &lt;- Url\n  urls &lt;- lapply(url1, paste, mm, \"-divvy-tripdata.zip\", sep = \"\")[[1]]\n  \n  ls &lt;- vector(mode = \"list\", length = length(mm))\n  names(ls) &lt;- paste(mm, \"-divvy-tripdata.csv\", sep = \"\")\n  \n  options(timeout = Timeout)\n  for (i in 1:length(ls)) {\n    ls[[i]] &lt;- tempfile(fileext = \".zip\")\n    download.file(urls[i], ls[[i]])\n    ls[[i]] &lt;- unzip(ls[[i]], names(ls)[i], exdir = tempdir()) |&gt; \n      data.table::fread(na.strings = c(\"\", NA))\n  }\n  options(timeout = 60)\n  \n  raw.tot_data &lt;- data.table::rbindlist(ls) |&gt; add.cols() |&gt; change.datatypes() |&gt; \n    data.table::setcolorder(c(1,2,13,3,4,16,14,15,5:12,17)) |&gt; \n    data.table::setkey(ride_id)\n  \n  invisible(gc(reset = T))\n  return(raw.tot_data)\n}\n\n\n\nWe call the resulting dataset ‘raw.tot_data’.\n\n\nurl &lt;- \"https://divvy-tripdata.s3.amazonaws.com/\"\nraw.tot_data &lt;- get.raw_data(url, Timeout = 5000)\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\n\n\n\n\nNote:\n\n\n\nThe datasets are quite large (more then 5 million obs. total). If you wish to reproduce this analysis keep in mind it might take much time, even more than 1 hour with non recent hardware."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#data-cleansing",
    "href": "posts/Case_study_Cyclistic/index.html#data-cleansing",
    "title": "Case study - Cyclistic",
    "section": "Data cleansing",
    "text": "Data cleansing\n\nAs next step we do some data cleansing. We must deal with missing and bad values, which the raw dataset has plenty of.\n\n\ncbind(miss_val = lapply(raw.tot_data, is.na) |&gt; lapply(sum))\n\n                   miss_val\nride_id            0       \nrideable_type      0       \nmember_casual      0       \nstarted_at         0       \nended_at           0       \nride_length        0       \nyear_month         0       \nweekday            0       \nstart_station_name 823167  \nstart_station_id   823164  \nend_station_name   878338  \nend_station_id     878338  \nstart_lat          0       \nstart_lng          0       \nend_lat            5036    \nend_lng            5036    \nride_dist          5036    \n\n\n\nWe use two additional custom functions to accomplish this task.\n\n\n\nCustom cleansing functions\nrm.outliers &lt;- function(data) {\n  if(!(\"data.table\" %in% class(data))) stop(\"dt is not a data.table object.\")\n  rl.upp_whisk &lt;- robustbase::adjboxStats(data[ride_length &gt; 0, ride_length])$stats[5]\n  df &lt;- data[-which(ride_length &lt; 0 | ride_length &gt; rl.upp_whisk\n                    | (ride_dist == 0 & start_station_name != end_station_name)\n                    | (ride_dist != 0 & start_station_name == end_station_name)\n                    | ride_dist %in% data[order(-ride_dist), head(.SD, 2)]$ride_dist)]\n  return(df)\n}\n\n\nrm.missing &lt;- function(data) {\n  if(!(\"data.table\" %in% class(data))) stop(\"dt is not a data.table object.\")\n  df &lt;- data[, grep(\"*station\", names(data)) := NULL]\n  df &lt;- df[complete.cases(df)]\n  return(df)\n}\n\n\n\nThis is what the two functions do:\n\nDrop obs. with ‘ride_length’ &lt; 0, since negative times don’t make sense.\nDrop obs. with ‘ride_length’ &gt; extreme of upper whisker of its boxplot adjusted for skewed distributions (see robustbase::adjboxStats() documentation).\nDrop obs. with ‘ride_dist’ = 0, and ‘start_station_name’ != ‘end_station_name’, since null distance only makes sense when start station and end station are the same.\nDrop obs. with ‘ride_dist’ != 0, and ‘start_station_name’ = ‘end_station_name’.\nDrop the two highest ‘ride_dist’ values, since max value is physically impossible, and the second highest one, while it might make sense in theory, is also an extremely far outlier compared to other values.\nDrop station IDs and names variables, since they have many non imputable values, and we won’t use those variables in the following analysis.\nDrop rows with missing ‘end_lat’, ‘end_lng’ and ‘ride_dist’ values.\n\n\n\n\n\n\n\nWarning!\n\n\n\nArbitrarily removing observations from datasets is bad practice in real case scenarios, and might lead to strongly biased results. Always confront with your team and project stakeholders before removing any observation. However this is only a fictional case study, and this is the best we can do here.\n\n\nWe now run the two custom functions, and assign the output to ‘tot_data’. This is the final dataset we’ll use throughout the rest of the analysis. It has 5.673.722 rows and 13 columns.\n\n\ntot_data &lt;- raw.tot_data |&gt; rm.outliers() |&gt; rm.missing()\nstr(tot_data)\n\nClasses 'data.table' and 'data.frame':  5673721 obs. of  13 variables:\n $ ride_id      : chr  \"00000123F60251E6\" \"000002EBE159AE82\" \"0000080D43BAA9E4\" \"00000B4F1F71F9C2\" ...\n $ rideable_type: Factor w/ 3 levels \"classic_bike\",..: 1 3 1 3 3 1 3 1 1 3 ...\n $ member_casual: Factor w/ 2 levels \"casual\",\"member\": 2 2 1 2 2 2 2 2 1 2 ...\n $ started_at   : POSIXct, format: \"2022-02-07 15:47:40\" \"2021-06-22 17:25:15\" ...\n $ ended_at     : POSIXct, format: \"2022-02-07 15:49:28\" \"2021-06-22 17:31:34\" ...\n $ ride_length  : num  108 379 2758 376 668 ...\n $ year_month   : Ord.factor w/ 12 levels \"Jun 2021\"&lt;\"Jul 2021\"&lt;..: 9 1 3 4 12 5 11 3 2 6 ...\n $ weekday      : Ord.factor w/ 7 levels \"Mon\"&lt;\"Tue\"&lt;\"Wed\"&lt;..: 1 2 7 3 4 7 1 5 3 2 ...\n $ start_lat    : num  41.9 41.9 41.9 41.9 41.9 ...\n $ start_lng    : num  -87.6 -87.6 -87.6 -87.7 -87.6 ...\n $ end_lat      : num  41.9 41.9 41.9 41.9 41.9 ...\n $ end_lng      : num  -87.6 -87.6 -87.6 -87.7 -87.6 ...\n $ ride_dist    : num  361 1581 467 830 2441 ...\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n - attr(*, \"sorted\")= chr \"ride_id\""
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#explorative-analysis",
    "href": "posts/Case_study_Cyclistic/index.html#explorative-analysis",
    "title": "Case study - Cyclistic",
    "section": "Explorative analysis",
    "text": "Explorative analysis\n\nNext step is a brief exploratory analysis. We start by looking at ‘ride_length’ and ‘ride_dist’ distributions, with boxplots grouped by ‘member_casual’ variable, being the main variable of interest for this case study. Again, we use a custom function to draw the boxplots, and we apply a custom ggplot2 theme.\n\n\n\nCustom boxplot function\nbox_plot &lt;- function(data, x, y, mult=1, Fill = \"thistle2\", outl.size = 0.2, Notch = F, Coef = 1.5, \n                     stat = \"mean\", Breaks = waiver(), Title = \"Title\", Subtitle = waiver(), \n                     x.lab = \"x-axis label\", y.lab = \"y-axis label\") {\n  \n  x = parse(text = x)\n  y = parse(text = y)\n  \n  box &lt;- ggplot2::ggplot(data, aes(x = eval(x), y = eval(y)*mult)) +\n    geom_boxplot(outlier.size = outl.size, fill = Fill, notch = Notch, coef = Coef) +\n    stat_summary(fun = stat) +\n    scale_y_continuous(breaks = Breaks) +\n    ggtitle(Title, Subtitle) +\n    xlab(x.lab) +\n    ylab(y.lab)\n  \n  return(box)\n}\n\n\n\n\nCustom ggplot2 theme\ntheme_update(\n  axis.text = element_text(size = 11),\n  legend.background = element_blank(),\n  panel.background = element_rect(fill = \"grey85\"),\n  panel.border = element_rect(colour = \"black\", fill = NA),\n  panel.grid = element_line(colour = \"whitesmoke\"),\n  panel.grid.minor.y = element_blank(),\n  panel.grid.major.x = element_blank(),\n  plot.title = element_text(hjust = 0.5),\n  plot.subtitle = element_text(face = \"italic\", hjust = 0.5),\n  strip.background = element_rect(colour = \"black\", fill = \"grey85\"),\n  strip.text = element_text(size = 10, face = \"bold\"),\n  title = element_text(size = 12, face = \"bold\")\n)\n\n\n\nBelow you can see the resulting plots:\n\n\nrl_box &lt;- box_plot(tot_data, \"member_casual\", \"ride_length\", mult = 1/60,\n                        Fill =  viridisLite::turbo(25)[c(5,16)], Notch = T, Breaks = seq(0,90,5), \n                        Title = \"Ride length boxplots\", x.lab = \"Membership\",\n                        y.lab = \"Ride length (min.)\")\nrd_box &lt;- box_plot(tot_data, \"member_casual\", \"ride_dist\", mult = 1/1000,\n                        Fill =  viridisLite::turbo(25)[c(5,16)], Notch = T, Breaks = seq(0,35,2.5), \n                        Title = \"Ride dist boxplots\", x.lab = \"Membership\", \n                        y.lab = \"Ride distance (km)\")\n\nrl_box + rd_box # patchwork composition\n\n\n\n\n\nAll the four distributions are heavily right skewed, with mean (black dot) &gt; median, long upper whiskers and many outliers on the upper side.\nLooking at ‘ride_length’ plot, we notice casuals group has higher variance than members one, since casuals box is wider. Also, all casuals boxplot statistics have higher values than their members counterparts. Hence it seems like casual customers have a tendency to take longer rides, and more diverse use cases for Cyclistic’s bikes. Conversely, the two ‘ride_dist’ boxplots look very similar, suggesting the two groups cover the same distance on average. Additional more formal analysis is required here in order to draw conclusions. This preliminar graphical analysis suggests members are more likely to use Cyclistic’s bikes for daily routine tasks, like commuting to work, compared to casual customers who occasionally rent them for leisure. Moreover, from the boxplots results, it’s clear that the current Cyclistic strategy doesn’t profit from ‘ride_length’, since members, being the most profitable group, take shorter rides.\nAs next step we compute some summary statistics for ‘ride_length’ and ‘rideable_type’ variables. We’ll use them later to draw additional visualizations. Again we write a custom function to do it, in order to save keystrokes.\n\n\n\nCustom summary function\nSumm &lt;- function(data, var, group_by, order_by = group_by) {\n  if(!(\"data.table\" %in% class(data))) stop(\"dt is not a data.table object.\")\n  \n  x = parse(text = var)\n  summ = data[\n    ,\n    .(obs = NROW(eval(x)), quart1 = quantile(eval(x), 0.25), mean = mean(eval(x)), \n      median = median(eval(x)), quart3 = quantile(eval(x), 0.75), iqr = IQR(eval(x)),\n      stdev = sd(eval(x))), \n    by = group_by\n  ] |&gt; data.table::setorderv(cols = order_by)\n  \n  return(summ)\n}\n\n\n\nrl_summ_by_ym &lt;- Summ(tot_data, \"ride_length\", c(\"member_casual\", \"year_month\"))\nrl_summ_by_wd &lt;- Summ(tot_data, \"ride_length\", c(\"member_casual\", \"weekday\"))\nrt_summ &lt;- tot_data[, .(obs = .N), by = c(\"member_casual\", \"rideable_type\")]\n\n\nrl_summ_by_ymrl_summ_by_wdrt_summ\n\n\n\nrl_summ_by_ym\n\n    member_casual year_month    obs quart1      mean median quart3    iqr\n 1:        casual   Jun 2021 343553  576.0 1283.0834    979   1673 1097.0\n 2:        casual   Jul 2021 413932  561.0 1245.7968    950   1619 1058.0\n 3:        casual   Aug 2021 388833  545.0 1213.8663    921   1575 1030.0\n 4:        casual   Sep 2021 344914  522.0 1178.2975    883   1525 1003.0\n 5:        casual   Oct 2021 244740  468.0 1078.4636    793   1389  921.0\n 6:        casual   Nov 2021 102833  395.0  918.5119    660   1162  767.0\n 7:        casual   Dec 2021  67023  387.5  892.8884    643   1132  744.5\n 8:        casual   Jan 2022  17810  363.0  805.6264    590   1008  645.0\n 9:        casual   Feb 2022  20405  387.0  880.3110    632   1110  723.0\n10:        casual   Mar 2022  84746  460.0 1127.8093    816   1489 1029.0\n11:        casual   Apr 2022 119640  452.0 1099.3256    795   1439  987.0\n12:        casual   May 2022 262577  501.0 1185.9838    875   1550 1049.0\n13:        member   Jun 2021 354758  373.0  817.9696    636   1075  702.0\n14:        member   Jul 2021 376543  365.0  802.9104    625   1052  687.0\n15:        member   Aug 2021 387775  357.0  793.4779    608   1037  680.0\n16:        member   Sep 2021 388197  343.0  770.8177    587   1002  659.0\n17:        member   Oct 2021 369407  307.0  698.4691    519    893  586.0\n18:        member   Nov 2021 249856  277.0  629.5316    461    787  510.0\n19:        member   Dec 2021 175515  275.0  620.9361    457    782  507.0\n20:        member   Jan 2022  84244  279.0  608.3621    448    753  474.0\n21:        member   Feb 2022  93157  275.0  616.9100    451    766  491.0\n22:        member   Mar 2022 191845  284.0  662.8775    482    839  555.0\n23:        member   Apr 2022 241999  277.0  651.5632    474    826  549.0\n24:        member   May 2022 349419  322.0  755.8451    564    983  661.0\n    member_casual year_month    obs quart1      mean median quart3    iqr\n        stdev\n 1: 1003.7459\n 2:  978.9981\n 3:  961.0095\n 4:  949.0889\n 5:  898.0429\n 6:  798.8552\n 7:  773.8583\n 8:  701.4893\n 9:  774.1430\n10:  956.9421\n11:  936.7498\n12:  985.0330\n13:  636.7139\n14:  630.3621\n15:  632.8731\n16:  620.8099\n17:  585.3930\n18:  540.6631\n19:  528.8499\n20:  519.2169\n21:  537.1952\n22:  575.2290\n23:  568.6321\n24:  632.8844\n        stdev\n\n\n\n\n\nrl_summ_by_wd\n\n    member_casual weekday    obs quart1      mean median  quart3     iqr\n 1:        casual     Mon 282856    495 1178.6815    865 1546.00 1051.00\n 2:        casual     Tue 272242    462 1066.9628    780 1360.00  898.00\n 3:        casual     Wed 272028    464 1057.6846    780 1350.00  886.00\n 4:        casual     Thu 293717    468 1061.7332    783 1352.00  884.00\n 5:        casual     Fri 342166    491 1119.2363    833 1445.75  954.75\n 6:        casual     Sat 511457    570 1270.2624    976 1667.00 1097.00\n 7:        casual     Sun 436540    574 1300.8242    996 1710.00 1136.00\n 8:        member     Mon 460809    308  708.8109    525  910.00  602.00\n 9:        member     Tue 519256    308  690.7078    520  887.00  579.00\n10:        member     Wed 507258    312  697.7463    528  897.00  585.00\n11:        member     Thu 496600    312  699.4487    528  901.00  589.00\n12:        member     Fri 454393    315  713.9517    537  921.00  606.00\n13:        member     Sat 435214    349  811.2369    614 1063.00  714.00\n14:        member     Sun 389185    342  815.4492    607 1074.00  732.00\n        stdev\n 1:  977.6261\n 2:  901.7131\n 3:  884.0924\n 4:  886.1289\n 5:  918.3581\n 6:  989.3016\n 7: 1018.5658\n 8:  595.1113\n 9:  565.8532\n10:  569.0408\n11:  572.8629\n12:  588.8620\n13:  665.4548\n14:  681.0527\n\n\n\n\n\nrt_summ\n\n   member_casual rideable_type     obs\n1:        member  classic_bike 1974193\n2:        member electric_bike 1288522\n3:        casual  classic_bike 1182884\n4:        casual   docked_bike  235153\n5:        casual electric_bike  992969\n\n\n\n\n\n\nAs final step of our exploratory data analysis, we perform Welch’s t-tests on ‘ride_length’ and ‘ride_dist’ variables means, with ‘member_casual’ being the grouping factor. We also compute Cohen’s D estimates to see the actual effect size of ‘member_casual’ on the other two variables.\n\n\n\n\n\n\nNote:\n\n\n\n\n\nSince we have a very large dataset, Welch’s tests will almost certainly reject the null hypothesis of equal population means for member and casual customers. That’s why we also estimate the actual effect sizes of ‘member_casual’ variable with Cohen’s d statistics.\n\n\n\n\n\nwelch_rl &lt;- t.test(ride_length ~ member_casual, data = tot_data)\nwelch_rd &lt;- t.test(ride_dist ~ member_casual, data = tot_data)\n\n\nwelch_rlwelch_rd\n\n\n\nwelch_rl\n\n\n    Welch Two Sample t-test\n\ndata:  ride_length by member_casual\nt = 630.05, df = 3806121, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group casual and group member is not equal to 0\n95 percent confidence interval:\n 440.0178 442.7640\nsample estimates:\nmean in group casual mean in group member \n           1171.2742             729.8833 \n\n\n\n\n\nwelch_rd\n\n\n    Welch Two Sample t-test\n\ndata:  ride_dist by member_casual\nt = 123.77, df = 5074284, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group casual and group member is not equal to 0\n95 percent confidence interval:\n 199.2456 205.6576\nsample estimates:\nmean in group casual mean in group member \n            2303.351             2100.899 \n\n\n\n\n\n\nAs expected, both tests reject the null hypothesis of equal population means. So let’s see how strong the relationships between ‘member_casuals’, and ‘ride_dist’ and ‘ride_length’ variables actually are. Relying on the two boxplots, we can assume a moderate to high effect size on ‘ride_length’ variable, and a small effect size on ‘ride_dist’ variable.\n\n\ncohensD_rl &lt;- rstatix::cohens_d(ride_length ~ member_casual, data = tot_data)\ncohensD_rd &lt;- rstatix::cohens_d(ride_dist ~ member_casual, data = tot_data)\n\n\ncohensD_rlcohensD_rd\n\n\n\ncohensD_rl\n\n# A tibble: 1 × 7\n  .y.         group1 group2 effsize      n1      n2 magnitude\n* &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;ord&gt;    \n1 ride_length casual member   0.552 2411006 3262715 moderate \n\n\n\n\n\ncohensD_rd\n\n# A tibble: 1 × 7\n  .y.       group1 group2 effsize      n1      n2 magnitude \n* &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt;   &lt;int&gt;   &lt;int&gt; &lt;ord&gt;     \n1 ride_dist casual member   0.105 2411006 3262715 negligible\n\n\n\n\n\n\nAs expected, our assumptions about effect sizes turned out correct. ‘member_casual’ has a moderate effect size on ‘ride_length’ variable, and a negligible effect on ‘ride_dist’. That’s more evidence supporting our previous findings about the two types of customers: casuals tend to take significantly longer rides and are more likely to use Cyclistic bikes for leisure."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#visualizations",
    "href": "posts/Case_study_Cyclistic/index.html#visualizations",
    "title": "Case study - Cyclistic",
    "section": "Visualizations",
    "text": "Visualizations\n\nLast step of our analysis consists of some additional visualizations about the total number of rides, and some maps.\n\n\n\nCustom visualiz. functions\ncol_plot &lt;- function(data, x, y, group, mult = 1, Legend = F, text_vjust = 0, text_col = \"black\", \n                     text_fface = \"bold\", text.size = 2.5, facet_nrow = 2, facet_strip = \"right\", \n                     Breaks = waiver(), col_man = rep(\"black\", facet_nrow), \n                     Fill_man = rep(\"thistle2\", facet_nrow), Title = \"Title\", Subtitle = waiver(), \n                     x.lab = \"x-axis label\", y.lab = \"y-axis label\", Round = 2, Angle = 0) {\n  x = parse(text = x)\n  y = parse(text = y)\n  group = parse(text = group)\n  \n  col &lt;- ggplot2::ggplot(data, aes(x = eval(x), y = eval(y)*mult)) +\n    geom_col(aes(color = eval(group), fill = eval(group)), show.legend = Legend) +\n    geom_text(aes(label = round(eval(y)*mult, Round)), \n              vjust = text_vjust, \n              colour = text_col, \n              fontface = text_fface, \n              size = text.size) +\n    facet_wrap(~eval(group), nrow = facet_nrow, strip.position = facet_strip) +\n    scale_y_continuous(breaks = Breaks) +\n    scale_color_manual(values = col_man) +\n    scale_fill_manual(values = Fill_man) +\n    theme(axis.text.x = element_text(angle = Angle)) +\n    ggtitle(Title, Subtitle) +\n    xlab(x.lab) +\n    ylab(y.lab)\n  \n  return(col)\n}\n\n\nmap_plot &lt;- function(data, lng, lat, group, Zoom = 11, Crop = F, Type = \"toner-lite\", transp = 0.05,\n                     dot.size = 0.85, col_man = \"black\", fill_man = \"black\", x.lab = \"longitude\",\n                     y.lab = \"latitude\", Title = \"map\") {\n  lng = parse(text = lng)\n  lat = parse(text = lat)\n  group = parse(text = group)\n  \n  map.lim = c(min(data[, eval(lng)]), min(data[, eval(lat)]), \n              max(data[, eval(lng)]), max(data[, eval(lat)]))\n  map = get_stamenmap(bbox = map.lim, zoom = Zoom, maptype = Type, crop = Crop)\n  \n  map.plot &lt;- ggmap::ggmap(map) +\n    geom_point(data = data, \n               aes(x = eval(lng), y = eval(lat), fill = eval(group), colour = eval(group)),\n               alpha = transp, size = dot.size) +\n    scale_color_manual(values = col_man) +\n    scale_fill_manual(values = fill_man) +\n    guides(colour = guide_legend(override.aes = list(alpha = 1, size = 1.5))) +\n    theme(legend.title = element_blank(), legend.position = c(0.80, 0.90),\n          legend.background = element_rect(colour = \"black\", size = 0.1)) +\n    xlab(x.lab) +\n    ylab(y.lab) +\n    ggtitle(label = Title)\n  \n  return(map.plot)\n}\n\n\n\nBelow you can look at column plots for number of rides by month and by day of week, both grouped by ‘member_casual’.\n\n\nnr.ym &lt;- col_plot(rl_summ_by_ym, \"year_month\", \"obs\", \"member_casual\", mult = 1/1000,\n                       text_vjust = 1.2, Breaks = seq(0, 500, 50), Title = \"Num. rides by month\",\n                       Fill_man = viridisLite::turbo(25)[c(5,16)], x.lab = \"Month\",\n                       y.lab = \"Num. rides (thousands)\", Round = 0, Angle = 90)\nnr.wd &lt;- col_plot(rl_summ_by_wd, \"weekday\", \"obs\", \"member_casual\", mult = 1/1000,\n                       text_vjust = 5, Breaks = seq(0, 500, 50), Title = \"Num. rides by weekday\",\n                       Fill_man = viridisLite::turbo(25)[c(5,16)], x.lab = \"Weekday\",\n                       y.lab = \"Num. rides (thousands)\", Round = 0)\n\nnr.ym + nr.wd #patchwork composition\n\n\n\n\n\nThese plots are quite interesting. The first one shows an obvious decline of the number of rides in cold seasons, for both casual and member customers, but the decline is steeper for casual riders. The second plot shows plain different trends in the number of rides by day of week: casuals do ride more during the weekend, while members have an opposite trend, with maximum number of rides during mid-week, and a decline towards the weekend. Those are pretty strong evidences that annual members mostly use Cyclistic bike sharing service to commute to work, while casuals rent bikes for leisure, mainly.\nWith the following column plot we also take a look at customers preferences about bike types.\n\n\nnr.rt &lt;- ggplot(rt_summ, aes(x = rideable_type, y = obs/1000)) +\n    geom_col(aes(fill = member_casual), position = \"dodge\", colour = \"black\") +\n    geom_text(\n      aes(group = member_casual, label = round(obs/1000)), \n      position = position_dodge2(width = 1),\n      vjust = 2, fontface = \"bold\"\n    ) +\n    scale_fill_manual(values = viridisLite::turbo(25)[c(5,16)]) +\n    theme(legend.position = c(.85, .85), legend.title = element_blank()) +\n    ylab(\"Num. rides (thousands)\") +\n    xlab(\"Bike type\") + \n    ggtitle(\"Num. rides by bike type\")\n\nnr.rt\n\n\n\n\n\nAs we can see, docked bikes ar by far the least popular category, and more important, those are only used by casual customers. It makes sense in accordance to previous findings, that annual members prefer the flexibility of non-docked bikes when commuting to work.\nEventually, we draw two maps paired with scatterplots of start and end coordinates.\n\n\nstart_map &lt;- map_plot(tot_data, \"start_lng\", \"start_lat\", \"member_casual\",\n                           col_man = viridisLite::turbo(25)[c(5,16)], \n                           fill_man = viridisLite::turbo(25)[c(5,16)], Title = \"Start map\")\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nend_map &lt;- map_plot(tot_data, \"end_lng\", \"end_lat\", \"member_casual\",\n                         col_man = viridisLite::turbo(25)[c(5,16)], \n                         fill_man = viridisLite::turbo(25)[c(5,16)], Title = \"End map\")\n\nstart_map + end_map #patchwork composition\n\n\n\n\n\nThe two maps clearly show that member customers are Chicago locals mainly, while casuals are more dispersed and more likely to be tourists."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#findings-and-suggestions",
    "href": "posts/Case_study_Cyclistic/index.html#findings-and-suggestions",
    "title": "Case study - Cyclistic",
    "section": "Findings and suggestions",
    "text": "Findings and suggestions\n\nBased on the previous analysis we can conclude that:\n\nMost member customers are Chicago locals and use Cyclistic bikes for daily routine activities, especially to commute to work.\nCasual customers are more dispersed, use Cyclistic bikes for leisure, and are more likely to be tourists.\n\nBeing the two groups fundamentally different between each other, it might be hard to convert casuals into members. However we can provide some potentially useful suggestions anyway:\n\nGather more data on casual riders with surveys. Ask about their home addresses, daily habits, work, etc.\nDevelop a dedicated social media campaign targeted at casual customers on weekends, specifically. A clever and low-cost idea might be asking the customers to share their weekend rides with Cyclistic on social media (in exchange for some discounts, etc.).\nAdopt a more flexible pricing structure, by offering something like a weekend-only membership. Also develop a plan to charge casual customers based on ‘ride_length’.\nConsider cutting investments on docked bikes, and focus on non-docked ones.\nConsider an expansion to Chicago neighborhoods.\nConsider partnerships with local enterprises, like offering discounted memberships for employees."
  },
  {
    "objectID": "posts/Case_study_Cyclistic/index.html#source-code",
    "href": "posts/Case_study_Cyclistic/index.html#source-code",
    "title": "Case study - Cyclistic",
    "section": "Source code",
    "text": "Source code\n\nFor a different take on this same case study, look at the source code I uploaded to github, where I also used renv for dependency management, and targets for workflow management. Follow the instructions in order to reproduce the analysis."
  },
  {
    "objectID": "posts/gis_crs/index.html",
    "href": "posts/gis_crs/index.html",
    "title": "GIS with R - Coordinate Reference Systems in R",
    "section": "",
    "text": "Geographic coordinate systems\n\nGeographic coordinate systems identify any location on the Earth’s surface using two values: longitude and latitude. Longitude is location in the East-West direction in angular distance from the Prime Meridian plane. Latitude is angular distance North or South of the equatorial plane. Distances in geographic CRSs are therefore not measured in meters.\nThe surface of the Earth in geographic coordinate systems is represented by a spherical or ellipsoidal surface.\n\nSpherical models: they assume that the Earth is a perfect sphere of a given radius. Spherical models have the advantage of simplicity but are rarely used because they are inaccurate: the Earth is not a sphere.\nEllipsoidal models: they are defined by two parameters: the equatorial radius and the polar radius. These are suitable because the Earth is compressed: the equatorial radius is around 11.5 km longer than the polar radius.\n\nEllipsoids are part of a wider component of CRSs: the datum. This contains information on what ellipsoid to use and the precise relationship between the Cartesian coordinates and location on the Earth’s surface. These additional details are stored in the respective arguments of PROJ4-string and WKT notations (see following sections for more details). There are two types of datum:\n\nLocal datum: in a local datum such as NAD83 the ellipsoidal surface is shifted to align with the surface at a particular location.\nGeocentric datum: in a geocentric datum, such as WGS84, the center is the Earth’s center of gravity and the accuracy of projections is not optimized for a specific location.\n\n\n\n\nProjected coordinate reference systems\n\nProjected CRSs are based on Cartesian coordinates on an implicitly flat surface. They have an origin, X and Y axes, and a linear unit of measurement such as meters. All projected CRSs are based on a geographic CRS, described in the previous section, and rely on map projections to convert the three-dimensional surface of the Earth into Easting and Northing (X and Y) values in a projected CRS.\nThis transition cannot be done without adding some distortion. Therefore, some properties of the Earth’s surface are distorted in this process, such as area, direction, distance, and shape. A projected coordinate system can preserve only one or two of those properties. Projections are often named based on a property they preserve:\n\nEqual-area: preserves area.\nAzimuthal: preserves direction.\nEquidistant: preserves distance.\nConformal: preserves local shape.\n\nThere are three main groups of projection types:\n\nConic: in a conic projection, the Earth’s surface is projected onto a cone along a single line of tangency or two lines of tangency. Distortions are minimized along the tangency lines and rise with the distance from those lines. Therefore, it is the best suited for maps of mid-latitude areas.\nCylindrical: a cylindrical projection maps the surface onto a cylinder. This projection could also be created by touching the Earth’s surface along a single line of tangency or two lines of tangency. Cylindrical projections are used most often when mapping the entire world.\nPlanar: a planar projection projects data onto a flat surface touching the globe at a point or along a line of tangency. It is typically used in mapping polar regions.\n\n\n\n\nHow to describe CRSs\n\nThree main ways to describe CRSs, are EPSG codes, PROJ4-strings and WKT formats.\n\nEPSG codes: EPSG codes are short numeric codes (4 to 6 digits) representing the parameters of a CRS. Each code refers to only one, well-defined, CRS.\nPROJ4-string: proj4strings are a compact way to identify a CRS, and are the primary output from many of the R spatial data packages, like raster and rgdal . They allow more flexibility when it comes to specifying different parameters such as the projection type, the datum and the ellipsoid. This also makes the proj4string approach more complicated than EPSG. Each proj4string is made up of many individual components:\n\n+proj: the projection.\n+zone: projection zone.\n+datum: the datum.\n+units: unit of measurement.\n+ellps: ellipsoid definition.\n\nWKT format: it’s a compact machine and human-readable representation of geometric objects. It defines elements of coordinate reference system (CRS) definitions using a combination of brackets [] and elements separated by commas (,).\n\n\n\n\nWhich CRS to use?\n\nThe question of which CRS is tricky, and there is rarely a ‘right’ answer: “There exist no all-purpose projections, all involve distortion when far from the center of the specified frame” (Bivand et al., 2013).\nFor geographic CRSs, the answer is often WGS84, not only for web mapping, but also because GPS datasets and thousands of raster and vector datasets are provided in this CRS by default. WGS84 is the most common CRS in the world, so it is worth knowing its EPSG code: 4326.\nWhat about when a projected CRS is required? In some cases, it is not something that we are free to decide: “often the choice of projection is made by a public mapping agency” (Bivand et al., 2013). This means that when working with local data sources, it is likely preferable to work with the CRS in which the data was provided, to ensure compatibility, even if the official CRS is not the most accurate.\nIn cases where an appropriate CRS is not immediately clear, the choice of CRS should depend on the properties that are most important to preserve in the subsequent maps and analysis.\n\n\n\nPackages\n\nBelow, we load the packages we’ll need later on.\n\nlibrary(rgdal)\nlibrary(sf)\n\n\n\n\nCRSs in R\n\nSpatial R packages support a wide range of CRSs and they use the long established PROJ library. Other than searching for EPSG codes online, an other quick way to find out about available CRSs is via the rgdal::make_EPSG() function.\n\ncrs &lt;- rgdal::make_EPSG()\ncrs\n\n\n\n  \n\n\n\nAs you can see, the function outputs a a data.frame with EPSG codes, CRSs names and corresponding PROJ4-strings.\n\n\n\n\n\n\nImportant:\n\n\n\nrgdal package will be retired by the end of 2023. Plan transition to sf/stars/terra functions using GDAL and PROJ.\n\n\nTo get a list of the available individual components of PROJ4-strings supported by the PROJ library, use sf::sf_proj_info() function. For example:\n\nsf::sf_proj_info(type = \"proj\") # list of available projections\n\n\n\n  \n\n\nsf::sf_proj_info(type = \"ellps\") # list of available ellipses\n\n\n\n  \n\n\n\nSee ?sf::sf_proj_info for more details.\nThe CRS of a vector data object can be retrieved using sf::st_crs() function. For example:\n\npt1 &lt;- c(50,50) |&gt; sf::st_point() \npt2 &lt;- c(55,55) |&gt; sf::st_point() \npts_sfc &lt;- sf::st_sfc(pt1, pt2, crs = 4326)\n\nsf::st_crs(pts_sfc)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nAs you can see, the function also returns the corresponding WKT notation. For the same reason, the function can be used to convert PROJ4-strings into WKT. For example, in the code below, we extract the PROJ4-string corresponding to WGS84 CRS, from the crs data.frame we created earlier, and convert it into WKT.\n\nWGS84_prj4 &lt;- crs[crs$code == 4326, \"prj4\"]\nsf::st_crs(WGS84_prj4)\n\nCoordinate Reference System:\n  User input: +proj=longlat +datum=WGS84 +no_defs +type=crs \n  wkt:\nGEOGCRS[\"unknown\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ID[\"EPSG\",6326]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433],\n        ID[\"EPSG\",8901]],\n    CS[ellipsoidal,2],\n        AXIS[\"longitude\",east,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"latitude\",north,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\n\n\nYou can set a new CRS for vector data objects with sf::st_set_crs() function. Below we change the CRS to Tokyo / UTM zone 51N.\n\npts_sfc &lt;- sf::st_set_crs(pts_sfc, crs[crs$note == \"Tokyo / UTM zone 51N\", \"code\"])\n\nWarning: st_crs&lt;- : replacing crs does not reproject data; use st_transform for\nthat\n\n\nThe warning message informs us that the function doesn’t transform data values from one CRS to another, only the CRS attribute.\nWith raster objects, use raster::projection() function to access and set CRS information.\n\n\n\n\n\n\nImportant\n\n\n\nRaster objects only support PROJ4-string notation currently."
  },
  {
    "objectID": "posts/gis_vector/index.html#packages",
    "href": "posts/gis_vector/index.html#packages",
    "title": "GIS with R - Vector data",
    "section": "Packages",
    "text": "Packages\n\nWe start by loading the needed packages.\n\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nThe sf package can be used to represent different geometry types in R. In addition to points, lines and polygons, there are 14 additional types of geometry that can be represented in sf which for now we ignore, but are detailed in the sf vignette. ggspatial package is a spatial data framework for ggplot2 ."
  },
  {
    "objectID": "posts/gis_vector/index.html#points",
    "href": "posts/gis_vector/index.html#points",
    "title": "GIS with R - Vector data",
    "section": "Points",
    "text": "Points\n\nPoints are the simplest spatial geometry, and are defined by a single XY coordinate. We can use sf::st_point() function to define a point.\n\npt &lt;- sf::st_point(c(10,20))\nattributes(pt)\n\n$class\n[1] \"XY\"    \"POINT\" \"sfg\"  \n\n\nattributes() function shows the different components of the object. As we can see, we got an sfg object of geometry type POINT as output. sfg is shorthand for Simple Feature Geometry objects, aka objects that only store XY coordinates and don’t have any information on the geographic coordinate system, datum or projection those coordinates are in, nor do they have any other attribute data associated with that point.\nIf we want to create an object with more than one point we need to aggregate points objects into a Simple Feature geometry Column (sfc) object, with sf:st_sfc() function.\n\n#### list of random points with coords between 40 and 75\nn &lt;- 10\nvec &lt;- c(55:75)\nls &lt;- vector(\"list\", n)\nls &lt;- lapply(1:n, sample, x = vec, size = 2) |&gt; lapply(st_point)\nnames(ls) &lt;- paste(\"point\", seq(1:n), sep = \"\")\n\n#### sfc object\npts_sfc &lt;- sf::st_sfc(ls)\nattributes(pts_sfc)\n\n$names\n [1] \"point1\"  \"point2\"  \"point3\"  \"point4\"  \"point5\"  \"point6\"  \"point7\" \n [8] \"point8\"  \"point9\"  \"point10\"\n\n$class\n[1] \"sfc_POINT\" \"sfc\"      \n\n$precision\n[1] 0\n\n$bbox\nxmin ymin xmax ymax \n  55   56   71   74 \n\n$crs\nCoordinate Reference System: NA\n\n$n_empty\n[1] 0\n\n\nAs you can see, sfc objects store additional attributes, like the bounding box (smallest rectangle that would encompass all elements in the data), decimal precision of coordinates, and CRS which is blank for now. Each reference system is defined by a corresponding EPSG code; see EPSG website, Spatial Reference website and GIS with R - CRSs in R post for more details. To add a CRS attribute to our sfc object, add a crs parameter to sf::st_sfc() function. For example, let’s say that we built the data based on longitude and latitude coordinates using a very commonly used geographic coordinate system called WGS84, which has an EPSG code of 4326.\n\npts_sfc &lt;- sf::st_sfc(ls, crs = 4326)\nattributes(pts_sfc)\n\n$names\n [1] \"point1\"  \"point2\"  \"point3\"  \"point4\"  \"point5\"  \"point6\"  \"point7\" \n [8] \"point8\"  \"point9\"  \"point10\"\n\n$class\n[1] \"sfc_POINT\" \"sfc\"      \n\n$precision\n[1] 0\n\n$bbox\nxmin ymin xmax ymax \n  55   56   71   74 \n\n$crs\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n$n_empty\n[1] 0\n\n\nAs you can see, we succesfully added a CRS attribute to our set of points. However, sfc objects don’t support custom attributes (not intended as R object attributes, but rather as characteristics associated with any given feature in a vector dataset).\nFor example, let’s say the points I just created represent the location of fast food restaurants, and for each restaurant we have informations about the corresponding brand. Since sfc objects can’t store additional attributes, we need to create a Simple Feature (sf) object with sf::st_sf() function which can integrate both the location and the attribute data. First we create the attribute dataset:\n\n#### Casual brand attributes from list of brands \nbrands &lt;- c(\"McDonald's\", \"Burger King\", \"America Graffiti\", \"KFC\", \"Pizza Hut\")\natt &lt;- data.frame(brand = sample(brands, n, replace = T))\natt\n\n\n\n  \n\n\n\nNow we can associate the attributes with location data:\n\npts_sf &lt;- sf::st_sf(att, geometry = pts_sfc)\npts_sf\n\n\n\n  \n\n\n\nFinally we can plot our data with ggplot2 package.\n\np &lt;- ggplot2::ggplot(pts_sf, aes(geometry = geometry, colour = brand)) +\n  geom_sf() +\n  labs(x = \"lon\", y = \"lat\")\np"
  },
  {
    "objectID": "posts/gis_vector/index.html#lines",
    "href": "posts/gis_vector/index.html#lines",
    "title": "GIS with R - Vector data",
    "section": "Lines",
    "text": "Lines\n\nWe can repeat the same process as above, but for line objects (LINESTRING geometry). We can create a single line with sf::st_linestring() function, which takes as input a matrix with at least two XY coordinates.\n\nline &lt;- matrix(c(10,12,20,15,22,21,10,6,5,16), ncol = 2, byrow = T) |&gt; \n  sf::st_linestring()\nline\n\nLINESTRING (10 12, 20 15, 22 21, 10 6, 5 16)\n\n\nBelow an sfc object with multiple lines and associated CRS code.\n\nl1 &lt;- matrix(c(10,10,10,12,14,16,15,10), ncol = 2, byrow = T) |&gt; sf::st_linestring()\nl2 &lt;- matrix(c(14,16,10,19), ncol = 2, byrow = T) |&gt; sf::st_linestring()\nl3 &lt;- matrix(c(10,12,5,13), ncol = 2, byrow = T) |&gt; sf::st_linestring()\n\nline_sfc &lt;- sf::st_sfc(l1,l2,l3, crs = 4326)\nline_sfc\n\nGeometry set for 3 features \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 5 ymin: 10 xmax: 15 ymax: 19\nGeodetic CRS:  WGS 84\n\n\nLINESTRING (10 10, 10 12, 14 16, 15 10)\n\n\nLINESTRING (14 16, 10 19)\n\n\nLINESTRING (10 12, 5 13)\n\n\nLet’s say these lines represent roads. We now create a dedicated attribute dataframe, with road names and speed limits, and attach them to lines data with sf::st_sf() .\n\nl_att &lt;- data.frame(road_name = c(\"Viale Garibaldi\", \"Via Voltaire\", \"Viale Pasolini\"), \n                    speed_limit = c(75, 50, 30))\nline_sf &lt;- sf::st_sf(l_att, line_sfc)\nline_sf\n\n\n\n  \n\n\n\nNow let’s plot the lines.\n\np1 &lt;- ggplot2::ggplot(line_sf, aes(geometry = line_sfc, colour = road_name)) +\n  geom_sf() +\n  labs(title = \"road_name\", x = \"lon\", y = \"lat\")\n\np2 &lt;- ggplot2::ggplot(line_sf, aes(geometry = line_sfc, colour = speed_limit)) +\n  geom_sf() +\n  labs(title = \"speed_limit\", x = \"lon\", y = \"lat\")\n\np1 / p2"
  },
  {
    "objectID": "posts/gis_vector/index.html#polygons",
    "href": "posts/gis_vector/index.html#polygons",
    "title": "GIS with R - Vector data",
    "section": "Polygons",
    "text": "Polygons\n\nPolygons are enclosed linestrings (first and last vertex are the same). You can create POLYGON geometries with sf::st_polygon() function. The function takes a list as an argument which contains one or more linestring matrices where the first and last rows are the same XY coordinate within a list element.\n\npol_list &lt;- matrix(c(10,10,10,20,20,20,20,10,10,10), ncol = 2, byrow = T) |&gt; list()\npol &lt;- sf::st_polygon(pol_list)\nattributes(pol)\n\n$class\n[1] \"XY\"      \"POLYGON\" \"sfg\"    \n\n\nYou can create more complex polygons by adding more polygon matrices to the list. For example, you can add a hole to the main polygon, by encompassing a smaller polygon.\n\npol_list[[2]] &lt;- matrix(c(15,15,15,16,16,16,16,15,15,15), ncol = 2, byrow = T)\npol_with_hole &lt;- sf::st_polygon(pol_list)\nplot(pol_with_hole, col = c(\"red3\"))\n\n\n\n\nLike for points and lines, you can create an sf object with multyple polygons, attributes and a CRS.\n\ntriangle &lt;- matrix(c(1,1,3,4,5,1,1,1), ncol = 2, byrow = T) |&gt; list() |&gt; sf::st_polygon()\nsquare &lt;- matrix(c(1,5,1,9,5,9,5,5,1,5), ncol = 2, byrow = T) |&gt; list() |&gt; sf::st_polygon()\npentagon &lt;- matrix(c(8,3,7,5,9,7,11,5,10,3,8,3), ncol = 2, byrow = T) |&gt; list() |&gt; sf::st_polygon()\n\npol_sfc &lt;- sf::st_sfc(triangle, square, pentagon, crs = 4326)\n\npol_att &lt;- data.frame(type = c(\"triangle\", \"square\", \"pentagon\"))\n \npol_sf &lt;- sf::st_sf(pol_att, pol_sfc)\npol_sf\n\n\n\n  \n\n\n\nBelow the three polygons plotted with ggplot2 package.\n\npolyplot &lt;- ggplot2::ggplot(pol_sf, aes(geometry = pol_sfc, fill = type)) +\n  geom_sf(show.legend = F) +\n  geom_sf_text(aes(label = type)) +\n  labs(x = \"lon\", y = \"lat\", title = \"Some polygons on 4326 CRS\")\npolyplot"
  },
  {
    "objectID": "posts/reg_diagn/index.html#linear-models-assumptions",
    "href": "posts/reg_diagn/index.html#linear-models-assumptions",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Linear models assumptions",
    "text": "Linear models assumptions\n\nWe start with a quick recap of OLS linear regression models assumptions. Linear models make a number of assumptions about the predictor variables, the response variables and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model. The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):\n\nLinearity: the basic assumption of the linear regression model, as the name suggests, is that of a linear relationship between the dependent and independent variables.\n\\(y_i = \\beta_1x_{1i}+ \\beta_2x_{2i} + ...+\\beta_nx_{ni} + \\epsilon_i\\)\nHere the linearity is only with respect to the parameters. Oddly enough, there’s no such restriction on the degree or form of the explanatory variables themselves. Those can be transforned as you like, for example taking the square or logarithm of one or more predictors; polynomials are also allowed.\nNull population mean of error terms: the error term accounts for the variation in the dependent variable that the independent variables do not explain. Random chance should determine the values of the error term. For your model to be unbiased, the average value of the error term must equal zero.\nPredictors uncorrelated with residuals: if an independent variable is correlated with the error term, we can use the independent variable to predict the error term, which violates the notion that the error term represents unpredictable random error. This assumption is also referred to as exogeneity. When this type of correlation exists, there is endogeneity. Violations of this assumption can occur because there is simultaneity between the independent and dependent variables, omitted variable bias, or measurement error in the independent variables.\nNo autocorrelation of residuals: the residuals in the linear regression model are assumed to be independently and identically distributed (i.i.d.). This implies that each error term is independent and unrelated to the other error terms. So, knowing one error term tells us nothing about the other(s). Also, all errors have the same distribution, the normal distribution (with zero mean and finite variance). The error term at a particular point in time should have no correlation with any of the past values. This makes the error terms equivalent to random noise that cannot be predicted. In the case of correlation between the residuals, the model’s accuracy is affected. Autocorrelation or serial correlation is a problem specific to regressions involving time-series.\nNo multicollinearity: the independent variables should not be correlated with each other. If there is a linear relationship between one or more explanatory variables, it adds to the complexity of the model without being able to delineate the impact of each explanatory variable on the response variable.\nNormality of residuals: the classic linear regression model assumes that the error term is normally distributed. If this assumption is violated, it is not a big problem, especially if we have a large number of observations. This is because the central limit theorem will apply if we have a large number of observations, implying that the sampling distribution will resemble normality irrespective of the parent distribution for large sample sizes. However, if the number of observations is small and the normality assumption is violated, the standard errors in your model’s output will be unreliable.\nHomoskedasticity: an important assumption of linear regression is that the error terms have the same variance across all observations. Unequal variance in the error terms is called heteroskedasticity and may be caused due to many reasons like the presence of outliers, or an incorrectly specified model. If we run the regression with heteroskedasticity present, the standard errors would be large and the model would have unreliable predictions."
  },
  {
    "objectID": "posts/reg_diagn/index.html#install-lindia",
    "href": "posts/reg_diagn/index.html#install-lindia",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Install lindia",
    "text": "Install lindia\n\nYou can install lindia from CRAN repository with:\n\n\ninstall.packages(\"lindia\")\n\n\nOr the developer version from GitHub:\n\n\ndevtools::install_github(\"yeukyul/lindia\")"
  },
  {
    "objectID": "posts/reg_diagn/index.html#packages",
    "href": "posts/reg_diagn/index.html#packages",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Packages",
    "text": "Packages\n\nWe start by loading the needed packages.\n\n\nlibrary(lindia)\nlibrary(car)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(olsrr)\nlibrary(corrplot)"
  },
  {
    "objectID": "posts/reg_diagn/index.html#dataset",
    "href": "posts/reg_diagn/index.html#dataset",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Dataset",
    "text": "Dataset\n\nWe’ll build a linear model on swiss R prebuilt dataset.\n\n\nswiss &lt;- datasets::swiss\nswiss\n\n\n\n  \n\n\n\n\nThe data frame (47 observations on 6 variables) has standardized fertility measure and socio-economic indicators for each of 47 French-speaking provinces of Switzerland at about 1888. Below a list of its variables:\n\nFertility: Ig, ‘common standardized fertility measure’.\nAgriculture: % of males involved in agriculture as occupation.\nExamination: % draftees receiving highest mark on army examination.\nEducation: % education beyond primary school for draftees.\nCatholic: % ‘catholic’ (as opposed to ‘protestant’).\nInfant.Mortality: live births who live less than 1 year.\n\nAll variables but ‘Fertility’ give proportions of the population."
  },
  {
    "objectID": "posts/reg_diagn/index.html#linear-model",
    "href": "posts/reg_diagn/index.html#linear-model",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Linear model",
    "text": "Linear model\n\nWe build a linear model with ‘Fertility’ as dependent variable, and other variables as predictors, with lm() function.\n\n\nmodel &lt;- lm(formula = Fertility ~ ., data = swiss)\nsummary(model)\n\n\nCall:\nlm(formula = Fertility ~ ., data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2743  -5.2617   0.5032   4.1198  15.3213 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***\nAgriculture      -0.17211    0.07030  -2.448  0.01873 *  \nExamination      -0.25801    0.25388  -1.016  0.31546    \nEducation        -0.87094    0.18303  -4.758 2.43e-05 ***\nCatholic          0.10412    0.03526   2.953  0.00519 ** \nInfant.Mortality  1.07705    0.38172   2.822  0.00734 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.165 on 41 degrees of freedom\nMultiple R-squared:  0.7067,    Adjusted R-squared:  0.671 \nF-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10\n\n\n\nIt seems like a decent model, with a sufficiently high R2 (0.671), and most estimates being statistically significant (with a confidence level of at least 95%), except for ‘Examination’ variable. So we just drop it and re-build the model.\n\n\nmodel &lt;- lm(formula = Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, data = swiss)\nsummary(model)\n\n\nCall:\nlm(formula = Fertility ~ Agriculture + Education + Catholic + \n    Infant.Mortality, data = swiss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-14.6765  -6.0522   0.7514   3.1664  16.1422 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      62.10131    9.60489   6.466 8.49e-08 ***\nAgriculture      -0.15462    0.06819  -2.267  0.02857 *  \nEducation        -0.98026    0.14814  -6.617 5.14e-08 ***\nCatholic          0.12467    0.02889   4.315 9.50e-05 ***\nInfant.Mortality  1.07844    0.38187   2.824  0.00722 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.168 on 42 degrees of freedom\nMultiple R-squared:  0.6993,    Adjusted R-squared:  0.6707 \nF-statistic: 24.42 on 4 and 42 DF,  p-value: 1.717e-10\n\n\n\nNow, all estimates are statistically significant, with lower standard errors, more narrow confidence intervals, lower p-values and a minimal loss of goodness of fit (R2 = 0.6707). Below we draw the regression plots for each regressor with a custom function.\n\n\n\nCustom function\nregr_p &lt;- function(Data, x, y) {\n  x = parse(text = x)\n  y = parse(text = y)\n  \n  model_summ &lt;- lm(data = Data, eval(y) ~ eval(x)) |&gt; summary()\n  \n  Sign &lt;- if(model_summ[[\"coefficients\"]][2] &gt;= 0) \"+\" else \"\"\n  Title = paste(\"Regression plot (X=\", x, \")\", sep = \"\")\n  Subtitle = paste(y, \"=\", model_summ[[\"coefficients\"]][1] |&gt; round(2), Sign, \n                   model_summ[[\"coefficients\"]][2] |&gt; round(2), \"*\", x, \"   \", \"\\nr-squared= \",\n                   model_summ[[\"r.squared\"]] |&gt; round(4), \"  Adj.r-squared= \",\n                   model_summ[[\"adj.r.squared\"]] |&gt; round(4))\n  \n  p &lt;- ggplot2::ggplot( data = Data, ggplot2::aes(x = eval(x), y = eval(y)) ) +\n    ggplot2::geom_point() +\n    ggplot2::geom_smooth(method = \"lm\", colour = \"red3\") +\n    ggplot2::ggtitle(label = Title, subtitle = Subtitle) +\n    ggplot2::xlab(x) + \n    ggplot2::ylab(y) +\n    ggplot2::theme(plot.title = ggplot2::element_text(hjust = 0.5, size = 10, face = \"bold\"), \n                   plot.subtitle = ggplot2::element_text(hjust = 0.5, size = 8))\n  return(p)\n}\n\nls &lt;- vector(\"list\", 4)\nnames(ls) &lt;- c(\"Agriculture\", \"Education\", \"Catholic\", \"Infant.Mortality\")\nfor (i in 1:length(ls)) {\n  ls[[i]] &lt;- regr_p(swiss, names(ls[i]), \"Fertility\")\n}\n\n(ls[[1]] + ls[[2]]) / (ls[[3]] + ls[[4]])\n\n\n\n\n\n\nFirst thing to notice is the presence of some ‘lonely’ outliers that may heavily influence some of the estimates (especially in the ‘Education’ and ‘Catholic’ models). We’ll address these outliers later. Except for the outliers, though, the linear model seems like a decent fit for all the four models. In particular, the ‘Education’ one has a relatively high R2 (0.4406), and a clear descending pattern. This result is in accordance with most modern fertility studies that confirm the existence of a negative correlation between fertility rates and education status. For example see Women’s education and fertility: results from 26 Demographic and Health Surveys. The ‘Agriculture’ model also exhibits a clear linear ascending pattern, regardless of the low R2. The other two models require further investigations. Anyway, empirical data confirms the existence of a positive correlation between fertility and religiosity (see Human fertility in relation to education, economy, religion, contraception, and family planning programs), and between fertility and infant mortality (see The influence of infant and child mortality on fertility in selected countries of the Asian and Pacific region), as wee see in the respective plots."
  },
  {
    "objectID": "posts/reg_diagn/index.html#diagnostics-with-lindia",
    "href": "posts/reg_diagn/index.html#diagnostics-with-lindia",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Diagnostics with lindia",
    "text": "Diagnostics with lindia\n\nlindia provides a set of streamlined functions that allow easy generation of linear regression diagnostic plots necessary for checking linear model assumptions. This package is meant for easy scheming of linear regression diagnostics, while preserving merits of “The Grammar of Graphics” as implemented in ggplot2.\nThe main function of lindia package is lindia::gg_diagnose() , which outputs an arranged grid of all the diagnostics plots available in lindia .\n\n\nlindia::gg_diagnose(model)\n\n\n\n\n\nAs you can see, the output includes:\n\nHistogram of residuals.\nResiduals vs. predictors plots.\nResidual vs. fit plots.\nQQ-plot.\nScale-location plot.\nResidual vs leverage plot.\nCook’s distance plot.\n\nYou can also add a boxcox plot to the grid, by adding boxcox=T parameter to the function call. If you need more flexibility in determining graphical elements and inclusion/exclusion of certain plots, set plot.all=F parameter in the function call. It will return a list of all plots, which the user can manipulate.\n\n\nl &lt;- lindia::gg_diagnose(model, plot.all = F)\nnames(l)\n\n [1] \"residual_hist\"    \"Agriculture\"      \"Education\"        \"Catholic\"        \n [5] \"Infant.Mortality\" \"res_fitted\"       \"qqplot\"           \"scalelocation\"   \n [9] \"resleverage\"      \"cooksd\"          \n\n\n\nYou can draw all the plots in a list with lindia::plot_all() function. For example, with the following code, we draw all the plots in ‘l’ list, except for residuals vs predictors plots.\n\n\nlindia::plot_all(l[-c(2:5)])\n\n\n\n\n\nYou can also draw the plots individually, via the corresponding functions:\n\ngg_reshist().\ngg_resfitted().\ngg_resX().\ngg_qqplot().\ngg_boxcox().\ngg_scalelocation().\ngg_resleverage().\ngg_cooksd().\n\nFor example, you can draw the histogram of residuals with lindia::gg_reshist() .\n\n\nlindia::gg_reshist(model)\n\n\n\n\n\nAll graphical styles returned by lindia graphing function can be overwritten by a call to ggplot::theme() (except lindia::gg_resX() and lindia::gg_diagnose(), which would return a list rather than a single ggplot object). For example:\n\n\nlindia::gg_reshist(model) + ggplot2::theme_dark()"
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-linearity-assumption",
    "href": "posts/reg_diagn/index.html#test-linearity-assumption",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test linearity assumption",
    "text": "Test linearity assumption\n\nThe easiest and most used method to assess if the linearity assumption holds is through residual plots (i.e. residuals vs fitted and residuals vs predictors plots). If the plots exhibit a clear non linear pattern, then the assumption of linearity is violated. It’s also important to check for outliers, since the OLS method is sensitive to them. Let’s take a closer look at the residuals plots of our swiss model.\n\n\nlindia::plot_all(l[c(2:6)])\n\n\n\n\n\nLinearity seems to hold reasonably well in ‘Agriculture’, ‘Infant.Mortality’. The points are randomly dispersed around the horizontal lines and don’t exhibit clear non linear patterns. Except for a weird pattern in the middle, also ‘Catholic’ plot exhibits a somewhat linear pattern. ‘Education’ plot has an extreme outlier, but seems to follow a linear trend too. Looking at the ‘Fitted values’ plots we also see a clear horizontal pattern, thus the linear model is a good fit overall. The linearity assumption holds reasonably well, here."
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-for-null-residuals-mean",
    "href": "posts/reg_diagn/index.html#test-for-null-residuals-mean",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test for null residuals mean",
    "text": "Test for null residuals mean\n\nThe points in the residuals plots are clearly centered around 0, so it’s safe to assume the assumption holds. Anyway we can compute the actual mean for an additional proof.\n\n\nmodel$residuals |&gt; mean()\n\n[1] 3.76029e-16\n\n\n\nAs you can see, the mean is so close to 0 that it’s practically null."
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-if-predictors-are-correlated-with-residuals",
    "href": "posts/reg_diagn/index.html#test-if-predictors-are-correlated-with-residuals",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test if predictors are correlated with residuals",
    "text": "Test if predictors are correlated with residuals\n\nThe residual plots already show there’s no clear correlation between residuals and predictors, but we can do an additional check with a correlation matrix. We can plot a nice correlation matrix with corrplot::corrplot() function.\n\n\ndata.frame(resid = model$residuals, swiss[,-c(1,3)]) |&gt; \n  cor() |&gt; \n  corrplot::corrplot(method = \"number\")\n\n\n\n\n\nAs you can see, the correlations between residuals and predictors are practically null, thus the assumption holds."
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-homoskedasticity-assumption",
    "href": "posts/reg_diagn/index.html#test-homoskedasticity-assumption",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test homoskedasticity assumption",
    "text": "Test homoskedasticity assumption\n\nThe residuals vs fitted plot is also a good tool to test for homoskedasticity, together with scale-location plot, which is an even better alternative when testing specifically for homoskedasticity.\n\n\nlindia::plot_all(l[c(6,8)], ncol = 2)\n\n\n\n\n\nThere’s no evident fan/conic shape in the first plot. The scale-location plot shows an approximately horizontal pattern (the plot is quite squeezed here), but is heavily influenced by a single outlier in the far left of the plot. Further investigation is needed. To get more info, we can run a formal heteroskedasticity test, like Breusch-Pagan test. If the p-value is less than the significance level, then the null hypotesis of homoskedasticity is rejected. You can run a BP test in R with car::ncvTest() function.\n\n\ncar::ncvTest(model)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.4687214, Df = 1, p = 0.49358\n\n\n\np &gt; 0.05, so we can’t reject the null hypotesis of homoskedasticity. Based on the previous tests, we can assume the homoskedasticity assumption actually holds."
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-no-perfect-multicollinearity-assumption",
    "href": "posts/reg_diagn/index.html#test-no-perfect-multicollinearity-assumption",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test no perfect multicollinearity assumption",
    "text": "Test no perfect multicollinearity assumption\n\nThe presence of multicollinearity can be assessed by checking the correlation between predictors, and/or by computing the Variance Inflation Factor (VIF) for each predictor. We already plotted a correlation matrix above; there’s a quite high correlation between ‘Education’ and ‘Agriculture’ variables. It makes sense that an higher percentual of individuals working in agriculture is tied to a lower education overall. So let’s see if the VIFs also suggest the presence of multicollinearity. We can compute the VIFs and associated tolerance values with olsrr::ols_vif_tol() function.\n\n\nolsrr::ols_vif_tol(model)\n\n\n\n  \n\n\n\n\n‘Agriculture’ variable actually has the highest VIF value, but since all VIFs are lower than 3, and all tolerance values are higher then 0.25 (those thresholds are the most commonly used ones, but they might vary a bit depending on how sensitive you want your multicollinearity test to be), we can exclude the presence of multicollinearity."
  },
  {
    "objectID": "posts/reg_diagn/index.html#test-normality-of-residuals-assumption",
    "href": "posts/reg_diagn/index.html#test-normality-of-residuals-assumption",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Test normality of residuals assumption",
    "text": "Test normality of residuals assumption\n\nThe easiest and most reliable way to test the normality assumption is by looking and the histogram of residuals and QQ plot.\n\n\nlindia::plot_all(l[c(1,7)], ncol = 2)\n\n\n\n\n\nThe standardized residuals seem to follow the normal distribution reasonably well here. For a more detailed qq plot, with a confidence interval included, you can use functions from other packages, like car::qqPlot() .\n\n\nqq &lt;- car::qqPlot(model$residuals, id=F)\n\n\n\n\n\nMost observations fall inside the confidence interval, thus we can assume the normality assumption holds."
  },
  {
    "objectID": "posts/reg_diagn/index.html#check-for-outliers-and-influential-observations",
    "href": "posts/reg_diagn/index.html#check-for-outliers-and-influential-observations",
    "title": "Linear regression diagnostics in R with lindia package",
    "section": "Check for outliers and influential observations",
    "text": "Check for outliers and influential observations\n\nPreviously, we observed some extreme data points that may negatively influence our model. The check for highly influential values, we can look at the residuals vs. leverage plot, and at the Cook’s distance plot.\n\n\nplot_all(l[c(9,10)], ncol = 2)\n\n\n\n\n\nBoth plots show highly influential data points, that when removed from the model, might greatly affect the parameters estimates. As we can see, there are at least 2 observations with high leverage, compared to other data points, and 3 observations (marked with the corresponding indexes) higher then the threshold level in the Cook’s plot. Let’s take a closer look at these data points.\n\n\nswiss[c(6,37,47),-3]\n\n\n\n  \n\n\n\n\n‘Porrentruy’ has a relatively low ‘Fertility’ for such high ‘Infant.Mortality’. The other two data points seem coherent with our previous model.\nGenerally speaking, arbitrarily removing data points is not good practice. Anyway, let’s try to re-build the model without these 3 observations, in order to see how influencial they are.\n\n\nmodel2 &lt;- lm(Fertility ~ Agriculture + Education + Catholic + Infant.Mortality, data = swiss[-c(6,37,47),])\nsummary(model2)\n\n\nCall:\nlm(formula = Fertility ~ Agriculture + Education + Catholic + \n    Infant.Mortality, data = swiss[-c(6, 37, 47), ])\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.9764  -4.7365   0.7061   3.7102  12.5382 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      55.67762    8.09634   6.877 3.16e-08 ***\nAgriculture      -0.20119    0.05787  -3.477  0.00126 ** \nEducation        -0.94425    0.12679  -7.447 5.25e-09 ***\nCatholic          0.13174    0.02494   5.283 5.11e-06 ***\nInfant.Mortality  1.50098    0.33565   4.472 6.52e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.925 on 39 degrees of freedom\nMultiple R-squared:  0.7683,    Adjusted R-squared:  0.7445 \nF-statistic: 32.32 on 4 and 39 DF,  p-value: 6.619e-12\n\n\n\nThe 3 data points were actually very influencial. This new model is significantly more accurate, with higher R2 and lower p-values (especially the ‘Agriculture’ and ‘Infant.Mortality’ ones got way lower)."
  },
  {
    "objectID": "posts/viridisLite/index.html#viridislite-package",
    "href": "posts/viridisLite/index.html#viridislite-package",
    "title": "Colorblind palettes in R with viridisLite package",
    "section": "viridisLite package",
    "text": "viridisLite package\n\nviridisLite is a lightweight version of viridis package. Both are meant to provide a series of color maps designed to improve graph readability for readers with common forms of vision impairment. Base viridis provides additional functionalities, while viridsLite is meant to be as lightweight and dependency-free as possible, for maximum compatibility with all the R ecosystems.\nThe package can be installed with:\n\n\ninstall.packages(\"viridisLite\")\n\n\nOr you can install the developer version with:\n\n\ndevtools::install_github(\"sjmgarnier/viridisLite\")"
  },
  {
    "objectID": "posts/viridisLite/index.html#palettes",
    "href": "posts/viridisLite/index.html#palettes",
    "title": "Colorblind palettes in R with viridisLite package",
    "section": "Palettes",
    "text": "Palettes\n\nviridisLite offers 8 color palettes to choose from (see Figure 1), via the corresponding convenience functions, useful when the scale must be passed as a function name. Each of those functions returns a character vector of color hex codes.\n\n\n\nFigure 1: viridisLite color palettes\n\n\nBelow you can see the general syntax of viridiLite convenience functions:\n\npalette_name(n, alpha = 1, begin = 0, end = 1, direction = 1)\n\nWhere:\n\nn: Number of colors to be in the palette .\nalpha: transparency, with 1 = no transparency, and 0 = max. transparency.\nbegin: starting hue.\nend: ending hue.\ndirection: order of colors. 1 = order from darkest to lightest. 0 = from lightest to darkest.\n\nSee the official documentation for more details. Following, I provide some examples."
  },
  {
    "objectID": "posts/viridisLite/index.html#packages",
    "href": "posts/viridisLite/index.html#packages",
    "title": "Colorblind palettes in R with viridisLite package",
    "section": "Packages",
    "text": "Packages\n\nWe start by loading the packages we’ll use throughout this tutorial.\n\n\nlibrary(ggplot2)\nlibrary(viridisLite)\nlibrary(scales)\nlibrary(patchwork)"
  },
  {
    "objectID": "posts/viridisLite/index.html#show-color-palettes",
    "href": "posts/viridisLite/index.html#show-color-palettes",
    "title": "Colorblind palettes in R with viridisLite package",
    "section": "Show color palettes",
    "text": "Show color palettes\n\nSince the output of viridisLite functions consists of a vector of hex codes, it’s quite hard to tell precisely which colors were actually picked. scales package provides a convenient and very easy way to visualize user generated palettes. See the following example.\n\n\npal &lt;- viridisLite::mako(25)\npal\n\n [1] \"#0B0405FF\" \"#170C14FF\" \"#211423FF\" \"#2B1C35FF\" \"#342346FF\" \"#3A2C58FF\"\n [7] \"#3E356BFF\" \"#413E7EFF\" \"#40498EFF\" \"#3B5698FF\" \"#38629DFF\" \"#366FA0FF\"\n[13] \"#357BA2FF\" \"#3486A5FF\" \"#3492A8FF\" \"#359EAAFF\" \"#38AAACFF\" \"#3FB6ADFF\"\n[19] \"#49C1ADFF\" \"#5BCDADFF\" \"#78D6AEFF\" \"#96DDB5FF\" \"#B1E4C2FF\" \"#C9ECD3FF\"\n[25] \"#DEF5E5FF\"\n\n\n\nWith scales::show_col() function we can plot ‘pal’ as a color matrix.\n\n\nscales::show_col(pal)"
  },
  {
    "objectID": "posts/viridisLite/index.html#plot-examples",
    "href": "posts/viridisLite/index.html#plot-examples",
    "title": "Colorblind palettes in R with viridisLite package",
    "section": "Plot examples",
    "text": "Plot examples\n\nBelow I provide some plot examples:\n\n\ndf &lt;- data.frame(x = rnorm(20000), y = rnorm(20000))\nvL_pals &lt;- c(\"viridis\", \"magma\", \"inferno\", \"plasma\", \"cividis\", \"rocket\",\n             \"mako\", \"turbo\")\n\nls &lt;- vector(mode = \"list\", length = length(vL_pals))\nnames(ls) &lt;- vL_pals\n\nfor (i in 1:length(vL_pals)) {\n  pal_parsed = parse(text = vL_pals[i])\n  \n  ls[[i]] &lt;- ggplot2::ggplot(df, aes(x, y)) +\n    stat_density2d(aes(fill = after_stat(level)), \n                   alpha = .5, geom = \"polygon\", n = 200) +\n    scale_fill_gradientn(colours = eval(pal_parsed)(n = 256)) +\n    ggtitle(label = vL_pals[i])\n}\n\n(ls[[1]] + ls[[2]]) / (ls[[3]] + ls[[4]]) / (ls[[5]] + ls[[6]]) / (ls[[7]] + ls[[8]]) # patchwork composition"
  }
]